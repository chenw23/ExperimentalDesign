{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31f0cef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# data preprocessing h5 to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f146a06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed7e9a9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = r'data\\data.h5'\n",
    "path_to = r'data\\result_data_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f4f1f18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 70  80  82 ... 106 109  82]\n",
      " [151 150 147 ... 193 183 184]\n",
      " [231 212 156 ...  88 110 152]\n",
      " ...\n",
      " [ 74  81  87 ... 188 187 187]\n",
      " [222 227 203 ... 136 136 134]\n",
      " [195 199 205 ...   6  15  38]]\n",
      "=====\n",
      "[0 0 2 ... 4 0 4]\n",
      "(28709,)\n",
      "[[ 70  80  82 ... 109  82   0]\n",
      " [151 150 147 ... 183 184   0]\n",
      " [231 212 156 ... 110 152   2]\n",
      " ...\n",
      " [ 74  81  87 ... 187 187   4]\n",
      " [222 227 203 ... 136 134   0]\n",
      " [195 199 205 ...  15  38   4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_store = pd.HDFStore(path)\n",
    "a = h5_store.keys()\n",
    "for key in a:\n",
    "    print(h5_store.get(key))\n",
    "with h5py.File(path, 'r') as f:\n",
    "    print(f[\"Training_pixel\"][()])\n",
    "    print(\"=====\")\n",
    "    print(f[\"Training_label\"][()])\n",
    "    train_pixel = f[\"Training_pixel\"][()]\n",
    "    train_label = f[\"Training_label\"][()]\n",
    "    train_label = np.transpose(train_label)\n",
    "    print(train_label.shape)\n",
    "result_train = np.c_[train_pixel, train_label]\n",
    "print(result_train)\n",
    "train_pixel.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "881ee904",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pixel_0', 'pixel_1', 'pixel_2', 'pixel_3', 'pixel_4', 'pixel_5', 'pixel_6', 'pixel_7', 'pixel_8', 'pixel_9', 'pixel_10', 'pixel_11', 'pixel_12', 'pixel_13', 'pixel_14', 'pixel_15', 'pixel_16', 'pixel_17', 'pixel_18', 'pixel_19', 'pixel_20', 'pixel_21', 'pixel_22', 'pixel_23', 'pixel_24', 'pixel_25', 'pixel_26', 'pixel_27', 'pixel_28', 'pixel_29', 'pixel_30', 'pixel_31', 'pixel_32', 'pixel_33', 'pixel_34', 'pixel_35', 'pixel_36', 'pixel_37', 'pixel_38', 'pixel_39', 'pixel_40', 'pixel_41', 'pixel_42', 'pixel_43', 'pixel_44', 'pixel_45', 'pixel_46', 'pixel_47', 'pixel_48', 'pixel_49', 'pixel_50', 'pixel_51', 'pixel_52', 'pixel_53', 'pixel_54', 'pixel_55', 'pixel_56', 'pixel_57', 'pixel_58', 'pixel_59', 'pixel_60', 'pixel_61', 'pixel_62', 'pixel_63', 'pixel_64', 'pixel_65', 'pixel_66', 'pixel_67', 'pixel_68', 'pixel_69', 'pixel_70', 'pixel_71', 'pixel_72', 'pixel_73', 'pixel_74', 'pixel_75', 'pixel_76', 'pixel_77', 'pixel_78', 'pixel_79', 'pixel_80', 'pixel_81', 'pixel_82', 'pixel_83', 'pixel_84', 'pixel_85', 'pixel_86', 'pixel_87', 'pixel_88', 'pixel_89', 'pixel_90', 'pixel_91', 'pixel_92', 'pixel_93', 'pixel_94', 'pixel_95', 'pixel_96', 'pixel_97', 'pixel_98', 'pixel_99', 'pixel_100', 'pixel_101', 'pixel_102', 'pixel_103', 'pixel_104', 'pixel_105', 'pixel_106', 'pixel_107', 'pixel_108', 'pixel_109', 'pixel_110', 'pixel_111', 'pixel_112', 'pixel_113', 'pixel_114', 'pixel_115', 'pixel_116', 'pixel_117', 'pixel_118', 'pixel_119', 'pixel_120', 'pixel_121', 'pixel_122', 'pixel_123', 'pixel_124', 'pixel_125', 'pixel_126', 'pixel_127', 'pixel_128', 'pixel_129', 'pixel_130', 'pixel_131', 'pixel_132', 'pixel_133', 'pixel_134', 'pixel_135', 'pixel_136', 'pixel_137', 'pixel_138', 'pixel_139', 'pixel_140', 'pixel_141', 'pixel_142', 'pixel_143', 'pixel_144', 'pixel_145', 'pixel_146', 'pixel_147', 'pixel_148', 'pixel_149', 'pixel_150', 'pixel_151', 'pixel_152', 'pixel_153', 'pixel_154', 'pixel_155', 'pixel_156', 'pixel_157', 'pixel_158', 'pixel_159', 'pixel_160', 'pixel_161', 'pixel_162', 'pixel_163', 'pixel_164', 'pixel_165', 'pixel_166', 'pixel_167', 'pixel_168', 'pixel_169', 'pixel_170', 'pixel_171', 'pixel_172', 'pixel_173', 'pixel_174', 'pixel_175', 'pixel_176', 'pixel_177', 'pixel_178', 'pixel_179', 'pixel_180', 'pixel_181', 'pixel_182', 'pixel_183', 'pixel_184', 'pixel_185', 'pixel_186', 'pixel_187', 'pixel_188', 'pixel_189', 'pixel_190', 'pixel_191', 'pixel_192', 'pixel_193', 'pixel_194', 'pixel_195', 'pixel_196', 'pixel_197', 'pixel_198', 'pixel_199', 'pixel_200', 'pixel_201', 'pixel_202', 'pixel_203', 'pixel_204', 'pixel_205', 'pixel_206', 'pixel_207', 'pixel_208', 'pixel_209', 'pixel_210', 'pixel_211', 'pixel_212', 'pixel_213', 'pixel_214', 'pixel_215', 'pixel_216', 'pixel_217', 'pixel_218', 'pixel_219', 'pixel_220', 'pixel_221', 'pixel_222', 'pixel_223', 'pixel_224', 'pixel_225', 'pixel_226', 'pixel_227', 'pixel_228', 'pixel_229', 'pixel_230', 'pixel_231', 'pixel_232', 'pixel_233', 'pixel_234', 'pixel_235', 'pixel_236', 'pixel_237', 'pixel_238', 'pixel_239', 'pixel_240', 'pixel_241', 'pixel_242', 'pixel_243', 'pixel_244', 'pixel_245', 'pixel_246', 'pixel_247', 'pixel_248', 'pixel_249', 'pixel_250', 'pixel_251', 'pixel_252', 'pixel_253', 'pixel_254', 'pixel_255', 'pixel_256', 'pixel_257', 'pixel_258', 'pixel_259', 'pixel_260', 'pixel_261', 'pixel_262', 'pixel_263', 'pixel_264', 'pixel_265', 'pixel_266', 'pixel_267', 'pixel_268', 'pixel_269', 'pixel_270', 'pixel_271', 'pixel_272', 'pixel_273', 'pixel_274', 'pixel_275', 'pixel_276', 'pixel_277', 'pixel_278', 'pixel_279', 'pixel_280', 'pixel_281', 'pixel_282', 'pixel_283', 'pixel_284', 'pixel_285', 'pixel_286', 'pixel_287', 'pixel_288', 'pixel_289', 'pixel_290', 'pixel_291', 'pixel_292', 'pixel_293', 'pixel_294', 'pixel_295', 'pixel_296', 'pixel_297', 'pixel_298', 'pixel_299', 'pixel_300', 'pixel_301', 'pixel_302', 'pixel_303', 'pixel_304', 'pixel_305', 'pixel_306', 'pixel_307', 'pixel_308', 'pixel_309', 'pixel_310', 'pixel_311', 'pixel_312', 'pixel_313', 'pixel_314', 'pixel_315', 'pixel_316', 'pixel_317', 'pixel_318', 'pixel_319', 'pixel_320', 'pixel_321', 'pixel_322', 'pixel_323', 'pixel_324', 'pixel_325', 'pixel_326', 'pixel_327', 'pixel_328', 'pixel_329', 'pixel_330', 'pixel_331', 'pixel_332', 'pixel_333', 'pixel_334', 'pixel_335', 'pixel_336', 'pixel_337', 'pixel_338', 'pixel_339', 'pixel_340', 'pixel_341', 'pixel_342', 'pixel_343', 'pixel_344', 'pixel_345', 'pixel_346', 'pixel_347', 'pixel_348', 'pixel_349', 'pixel_350', 'pixel_351', 'pixel_352', 'pixel_353', 'pixel_354', 'pixel_355', 'pixel_356', 'pixel_357', 'pixel_358', 'pixel_359', 'pixel_360', 'pixel_361', 'pixel_362', 'pixel_363', 'pixel_364', 'pixel_365', 'pixel_366', 'pixel_367', 'pixel_368', 'pixel_369', 'pixel_370', 'pixel_371', 'pixel_372', 'pixel_373', 'pixel_374', 'pixel_375', 'pixel_376', 'pixel_377', 'pixel_378', 'pixel_379', 'pixel_380', 'pixel_381', 'pixel_382', 'pixel_383', 'pixel_384', 'pixel_385', 'pixel_386', 'pixel_387', 'pixel_388', 'pixel_389', 'pixel_390', 'pixel_391', 'pixel_392', 'pixel_393', 'pixel_394', 'pixel_395', 'pixel_396', 'pixel_397', 'pixel_398', 'pixel_399', 'pixel_400', 'pixel_401', 'pixel_402', 'pixel_403', 'pixel_404', 'pixel_405', 'pixel_406', 'pixel_407', 'pixel_408', 'pixel_409', 'pixel_410', 'pixel_411', 'pixel_412', 'pixel_413', 'pixel_414', 'pixel_415', 'pixel_416', 'pixel_417', 'pixel_418', 'pixel_419', 'pixel_420', 'pixel_421', 'pixel_422', 'pixel_423', 'pixel_424', 'pixel_425', 'pixel_426', 'pixel_427', 'pixel_428', 'pixel_429', 'pixel_430', 'pixel_431', 'pixel_432', 'pixel_433', 'pixel_434', 'pixel_435', 'pixel_436', 'pixel_437', 'pixel_438', 'pixel_439', 'pixel_440', 'pixel_441', 'pixel_442', 'pixel_443', 'pixel_444', 'pixel_445', 'pixel_446', 'pixel_447', 'pixel_448', 'pixel_449', 'pixel_450', 'pixel_451', 'pixel_452', 'pixel_453', 'pixel_454', 'pixel_455', 'pixel_456', 'pixel_457', 'pixel_458', 'pixel_459', 'pixel_460', 'pixel_461', 'pixel_462', 'pixel_463', 'pixel_464', 'pixel_465', 'pixel_466', 'pixel_467', 'pixel_468', 'pixel_469', 'pixel_470', 'pixel_471', 'pixel_472', 'pixel_473', 'pixel_474', 'pixel_475', 'pixel_476', 'pixel_477', 'pixel_478', 'pixel_479', 'pixel_480', 'pixel_481', 'pixel_482', 'pixel_483', 'pixel_484', 'pixel_485', 'pixel_486', 'pixel_487', 'pixel_488', 'pixel_489', 'pixel_490', 'pixel_491', 'pixel_492', 'pixel_493', 'pixel_494', 'pixel_495', 'pixel_496', 'pixel_497', 'pixel_498', 'pixel_499', 'pixel_500', 'pixel_501', 'pixel_502', 'pixel_503', 'pixel_504', 'pixel_505', 'pixel_506', 'pixel_507', 'pixel_508', 'pixel_509', 'pixel_510', 'pixel_511', 'pixel_512', 'pixel_513', 'pixel_514', 'pixel_515', 'pixel_516', 'pixel_517', 'pixel_518', 'pixel_519', 'pixel_520', 'pixel_521', 'pixel_522', 'pixel_523', 'pixel_524', 'pixel_525', 'pixel_526', 'pixel_527', 'pixel_528', 'pixel_529', 'pixel_530', 'pixel_531', 'pixel_532', 'pixel_533', 'pixel_534', 'pixel_535', 'pixel_536', 'pixel_537', 'pixel_538', 'pixel_539', 'pixel_540', 'pixel_541', 'pixel_542', 'pixel_543', 'pixel_544', 'pixel_545', 'pixel_546', 'pixel_547', 'pixel_548', 'pixel_549', 'pixel_550', 'pixel_551', 'pixel_552', 'pixel_553', 'pixel_554', 'pixel_555', 'pixel_556', 'pixel_557', 'pixel_558', 'pixel_559', 'pixel_560', 'pixel_561', 'pixel_562', 'pixel_563', 'pixel_564', 'pixel_565', 'pixel_566', 'pixel_567', 'pixel_568', 'pixel_569', 'pixel_570', 'pixel_571', 'pixel_572', 'pixel_573', 'pixel_574', 'pixel_575', 'pixel_576', 'pixel_577', 'pixel_578', 'pixel_579', 'pixel_580', 'pixel_581', 'pixel_582', 'pixel_583', 'pixel_584', 'pixel_585', 'pixel_586', 'pixel_587', 'pixel_588', 'pixel_589', 'pixel_590', 'pixel_591', 'pixel_592', 'pixel_593', 'pixel_594', 'pixel_595', 'pixel_596', 'pixel_597', 'pixel_598', 'pixel_599', 'pixel_600', 'pixel_601', 'pixel_602', 'pixel_603', 'pixel_604', 'pixel_605', 'pixel_606', 'pixel_607', 'pixel_608', 'pixel_609', 'pixel_610', 'pixel_611', 'pixel_612', 'pixel_613', 'pixel_614', 'pixel_615', 'pixel_616', 'pixel_617', 'pixel_618', 'pixel_619', 'pixel_620', 'pixel_621', 'pixel_622', 'pixel_623', 'pixel_624', 'pixel_625', 'pixel_626', 'pixel_627', 'pixel_628', 'pixel_629', 'pixel_630', 'pixel_631', 'pixel_632', 'pixel_633', 'pixel_634', 'pixel_635', 'pixel_636', 'pixel_637', 'pixel_638', 'pixel_639', 'pixel_640', 'pixel_641', 'pixel_642', 'pixel_643', 'pixel_644', 'pixel_645', 'pixel_646', 'pixel_647', 'pixel_648', 'pixel_649', 'pixel_650', 'pixel_651', 'pixel_652', 'pixel_653', 'pixel_654', 'pixel_655', 'pixel_656', 'pixel_657', 'pixel_658', 'pixel_659', 'pixel_660', 'pixel_661', 'pixel_662', 'pixel_663', 'pixel_664', 'pixel_665', 'pixel_666', 'pixel_667', 'pixel_668', 'pixel_669', 'pixel_670', 'pixel_671', 'pixel_672', 'pixel_673', 'pixel_674', 'pixel_675', 'pixel_676', 'pixel_677', 'pixel_678', 'pixel_679', 'pixel_680', 'pixel_681', 'pixel_682', 'pixel_683', 'pixel_684', 'pixel_685', 'pixel_686', 'pixel_687', 'pixel_688', 'pixel_689', 'pixel_690', 'pixel_691', 'pixel_692', 'pixel_693', 'pixel_694', 'pixel_695', 'pixel_696', 'pixel_697', 'pixel_698', 'pixel_699', 'pixel_700', 'pixel_701', 'pixel_702', 'pixel_703', 'pixel_704', 'pixel_705', 'pixel_706', 'pixel_707', 'pixel_708', 'pixel_709', 'pixel_710', 'pixel_711', 'pixel_712', 'pixel_713', 'pixel_714', 'pixel_715', 'pixel_716', 'pixel_717', 'pixel_718', 'pixel_719', 'pixel_720', 'pixel_721', 'pixel_722', 'pixel_723', 'pixel_724', 'pixel_725', 'pixel_726', 'pixel_727', 'pixel_728', 'pixel_729', 'pixel_730', 'pixel_731', 'pixel_732', 'pixel_733', 'pixel_734', 'pixel_735', 'pixel_736', 'pixel_737', 'pixel_738', 'pixel_739', 'pixel_740', 'pixel_741', 'pixel_742', 'pixel_743', 'pixel_744', 'pixel_745', 'pixel_746', 'pixel_747', 'pixel_748', 'pixel_749', 'pixel_750', 'pixel_751', 'pixel_752', 'pixel_753', 'pixel_754', 'pixel_755', 'pixel_756', 'pixel_757', 'pixel_758', 'pixel_759', 'pixel_760', 'pixel_761', 'pixel_762', 'pixel_763', 'pixel_764', 'pixel_765', 'pixel_766', 'pixel_767', 'pixel_768', 'pixel_769', 'pixel_770', 'pixel_771', 'pixel_772', 'pixel_773', 'pixel_774', 'pixel_775', 'pixel_776', 'pixel_777', 'pixel_778', 'pixel_779', 'pixel_780', 'pixel_781', 'pixel_782', 'pixel_783', 'pixel_784', 'pixel_785', 'pixel_786', 'pixel_787', 'pixel_788', 'pixel_789', 'pixel_790', 'pixel_791', 'pixel_792', 'pixel_793', 'pixel_794', 'pixel_795', 'pixel_796', 'pixel_797', 'pixel_798', 'pixel_799', 'pixel_800', 'pixel_801', 'pixel_802', 'pixel_803', 'pixel_804', 'pixel_805', 'pixel_806', 'pixel_807', 'pixel_808', 'pixel_809', 'pixel_810', 'pixel_811', 'pixel_812', 'pixel_813', 'pixel_814', 'pixel_815', 'pixel_816', 'pixel_817', 'pixel_818', 'pixel_819', 'pixel_820', 'pixel_821', 'pixel_822', 'pixel_823', 'pixel_824', 'pixel_825', 'pixel_826', 'pixel_827', 'pixel_828', 'pixel_829', 'pixel_830', 'pixel_831', 'pixel_832', 'pixel_833', 'pixel_834', 'pixel_835', 'pixel_836', 'pixel_837', 'pixel_838', 'pixel_839', 'pixel_840', 'pixel_841', 'pixel_842', 'pixel_843', 'pixel_844', 'pixel_845', 'pixel_846', 'pixel_847', 'pixel_848', 'pixel_849', 'pixel_850', 'pixel_851', 'pixel_852', 'pixel_853', 'pixel_854', 'pixel_855', 'pixel_856', 'pixel_857', 'pixel_858', 'pixel_859', 'pixel_860', 'pixel_861', 'pixel_862', 'pixel_863', 'pixel_864', 'pixel_865', 'pixel_866', 'pixel_867', 'pixel_868', 'pixel_869', 'pixel_870', 'pixel_871', 'pixel_872', 'pixel_873', 'pixel_874', 'pixel_875', 'pixel_876', 'pixel_877', 'pixel_878', 'pixel_879', 'pixel_880', 'pixel_881', 'pixel_882', 'pixel_883', 'pixel_884', 'pixel_885', 'pixel_886', 'pixel_887', 'pixel_888', 'pixel_889', 'pixel_890', 'pixel_891', 'pixel_892', 'pixel_893', 'pixel_894', 'pixel_895', 'pixel_896', 'pixel_897', 'pixel_898', 'pixel_899', 'pixel_900', 'pixel_901', 'pixel_902', 'pixel_903', 'pixel_904', 'pixel_905', 'pixel_906', 'pixel_907', 'pixel_908', 'pixel_909', 'pixel_910', 'pixel_911', 'pixel_912', 'pixel_913', 'pixel_914', 'pixel_915', 'pixel_916', 'pixel_917', 'pixel_918', 'pixel_919', 'pixel_920', 'pixel_921', 'pixel_922', 'pixel_923', 'pixel_924', 'pixel_925', 'pixel_926', 'pixel_927', 'pixel_928', 'pixel_929', 'pixel_930', 'pixel_931', 'pixel_932', 'pixel_933', 'pixel_934', 'pixel_935', 'pixel_936', 'pixel_937', 'pixel_938', 'pixel_939', 'pixel_940', 'pixel_941', 'pixel_942', 'pixel_943', 'pixel_944', 'pixel_945', 'pixel_946', 'pixel_947', 'pixel_948', 'pixel_949', 'pixel_950', 'pixel_951', 'pixel_952', 'pixel_953', 'pixel_954', 'pixel_955', 'pixel_956', 'pixel_957', 'pixel_958', 'pixel_959', 'pixel_960', 'pixel_961', 'pixel_962', 'pixel_963', 'pixel_964', 'pixel_965', 'pixel_966', 'pixel_967', 'pixel_968', 'pixel_969', 'pixel_970', 'pixel_971', 'pixel_972', 'pixel_973', 'pixel_974', 'pixel_975', 'pixel_976', 'pixel_977', 'pixel_978', 'pixel_979', 'pixel_980', 'pixel_981', 'pixel_982', 'pixel_983', 'pixel_984', 'pixel_985', 'pixel_986', 'pixel_987', 'pixel_988', 'pixel_989', 'pixel_990', 'pixel_991', 'pixel_992', 'pixel_993', 'pixel_994', 'pixel_995', 'pixel_996', 'pixel_997', 'pixel_998', 'pixel_999', 'pixel_1000', 'pixel_1001', 'pixel_1002', 'pixel_1003', 'pixel_1004', 'pixel_1005', 'pixel_1006', 'pixel_1007', 'pixel_1008', 'pixel_1009', 'pixel_1010', 'pixel_1011', 'pixel_1012', 'pixel_1013', 'pixel_1014', 'pixel_1015', 'pixel_1016', 'pixel_1017', 'pixel_1018', 'pixel_1019', 'pixel_1020', 'pixel_1021', 'pixel_1022', 'pixel_1023', 'pixel_1024', 'pixel_1025', 'pixel_1026', 'pixel_1027', 'pixel_1028', 'pixel_1029', 'pixel_1030', 'pixel_1031', 'pixel_1032', 'pixel_1033', 'pixel_1034', 'pixel_1035', 'pixel_1036', 'pixel_1037', 'pixel_1038', 'pixel_1039', 'pixel_1040', 'pixel_1041', 'pixel_1042', 'pixel_1043', 'pixel_1044', 'pixel_1045', 'pixel_1046', 'pixel_1047', 'pixel_1048', 'pixel_1049', 'pixel_1050', 'pixel_1051', 'pixel_1052', 'pixel_1053', 'pixel_1054', 'pixel_1055', 'pixel_1056', 'pixel_1057', 'pixel_1058', 'pixel_1059', 'pixel_1060', 'pixel_1061', 'pixel_1062', 'pixel_1063', 'pixel_1064', 'pixel_1065', 'pixel_1066', 'pixel_1067', 'pixel_1068', 'pixel_1069', 'pixel_1070', 'pixel_1071', 'pixel_1072', 'pixel_1073', 'pixel_1074', 'pixel_1075', 'pixel_1076', 'pixel_1077', 'pixel_1078', 'pixel_1079', 'pixel_1080', 'pixel_1081', 'pixel_1082', 'pixel_1083', 'pixel_1084', 'pixel_1085', 'pixel_1086', 'pixel_1087', 'pixel_1088', 'pixel_1089', 'pixel_1090', 'pixel_1091', 'pixel_1092', 'pixel_1093', 'pixel_1094', 'pixel_1095', 'pixel_1096', 'pixel_1097', 'pixel_1098', 'pixel_1099', 'pixel_1100', 'pixel_1101', 'pixel_1102', 'pixel_1103', 'pixel_1104', 'pixel_1105', 'pixel_1106', 'pixel_1107', 'pixel_1108', 'pixel_1109', 'pixel_1110', 'pixel_1111', 'pixel_1112', 'pixel_1113', 'pixel_1114', 'pixel_1115', 'pixel_1116', 'pixel_1117', 'pixel_1118', 'pixel_1119', 'pixel_1120', 'pixel_1121', 'pixel_1122', 'pixel_1123', 'pixel_1124', 'pixel_1125', 'pixel_1126', 'pixel_1127', 'pixel_1128', 'pixel_1129', 'pixel_1130', 'pixel_1131', 'pixel_1132', 'pixel_1133', 'pixel_1134', 'pixel_1135', 'pixel_1136', 'pixel_1137', 'pixel_1138', 'pixel_1139', 'pixel_1140', 'pixel_1141', 'pixel_1142', 'pixel_1143', 'pixel_1144', 'pixel_1145', 'pixel_1146', 'pixel_1147', 'pixel_1148', 'pixel_1149', 'pixel_1150', 'pixel_1151', 'pixel_1152', 'pixel_1153', 'pixel_1154', 'pixel_1155', 'pixel_1156', 'pixel_1157', 'pixel_1158', 'pixel_1159', 'pixel_1160', 'pixel_1161', 'pixel_1162', 'pixel_1163', 'pixel_1164', 'pixel_1165', 'pixel_1166', 'pixel_1167', 'pixel_1168', 'pixel_1169', 'pixel_1170', 'pixel_1171', 'pixel_1172', 'pixel_1173', 'pixel_1174', 'pixel_1175', 'pixel_1176', 'pixel_1177', 'pixel_1178', 'pixel_1179', 'pixel_1180', 'pixel_1181', 'pixel_1182', 'pixel_1183', 'pixel_1184', 'pixel_1185', 'pixel_1186', 'pixel_1187', 'pixel_1188', 'pixel_1189', 'pixel_1190', 'pixel_1191', 'pixel_1192', 'pixel_1193', 'pixel_1194', 'pixel_1195', 'pixel_1196', 'pixel_1197', 'pixel_1198', 'pixel_1199', 'pixel_1200', 'pixel_1201', 'pixel_1202', 'pixel_1203', 'pixel_1204', 'pixel_1205', 'pixel_1206', 'pixel_1207', 'pixel_1208', 'pixel_1209', 'pixel_1210', 'pixel_1211', 'pixel_1212', 'pixel_1213', 'pixel_1214', 'pixel_1215', 'pixel_1216', 'pixel_1217', 'pixel_1218', 'pixel_1219', 'pixel_1220', 'pixel_1221', 'pixel_1222', 'pixel_1223', 'pixel_1224', 'pixel_1225', 'pixel_1226', 'pixel_1227', 'pixel_1228', 'pixel_1229', 'pixel_1230', 'pixel_1231', 'pixel_1232', 'pixel_1233', 'pixel_1234', 'pixel_1235', 'pixel_1236', 'pixel_1237', 'pixel_1238', 'pixel_1239', 'pixel_1240', 'pixel_1241', 'pixel_1242', 'pixel_1243', 'pixel_1244', 'pixel_1245', 'pixel_1246', 'pixel_1247', 'pixel_1248', 'pixel_1249', 'pixel_1250', 'pixel_1251', 'pixel_1252', 'pixel_1253', 'pixel_1254', 'pixel_1255', 'pixel_1256', 'pixel_1257', 'pixel_1258', 'pixel_1259', 'pixel_1260', 'pixel_1261', 'pixel_1262', 'pixel_1263', 'pixel_1264', 'pixel_1265', 'pixel_1266', 'pixel_1267', 'pixel_1268', 'pixel_1269', 'pixel_1270', 'pixel_1271', 'pixel_1272', 'pixel_1273', 'pixel_1274', 'pixel_1275', 'pixel_1276', 'pixel_1277', 'pixel_1278', 'pixel_1279', 'pixel_1280', 'pixel_1281', 'pixel_1282', 'pixel_1283', 'pixel_1284', 'pixel_1285', 'pixel_1286', 'pixel_1287', 'pixel_1288', 'pixel_1289', 'pixel_1290', 'pixel_1291', 'pixel_1292', 'pixel_1293', 'pixel_1294', 'pixel_1295', 'pixel_1296', 'pixel_1297', 'pixel_1298', 'pixel_1299', 'pixel_1300', 'pixel_1301', 'pixel_1302', 'pixel_1303', 'pixel_1304', 'pixel_1305', 'pixel_1306', 'pixel_1307', 'pixel_1308', 'pixel_1309', 'pixel_1310', 'pixel_1311', 'pixel_1312', 'pixel_1313', 'pixel_1314', 'pixel_1315', 'pixel_1316', 'pixel_1317', 'pixel_1318', 'pixel_1319', 'pixel_1320', 'pixel_1321', 'pixel_1322', 'pixel_1323', 'pixel_1324', 'pixel_1325', 'pixel_1326', 'pixel_1327', 'pixel_1328', 'pixel_1329', 'pixel_1330', 'pixel_1331', 'pixel_1332', 'pixel_1333', 'pixel_1334', 'pixel_1335', 'pixel_1336', 'pixel_1337', 'pixel_1338', 'pixel_1339', 'pixel_1340', 'pixel_1341', 'pixel_1342', 'pixel_1343', 'pixel_1344', 'pixel_1345', 'pixel_1346', 'pixel_1347', 'pixel_1348', 'pixel_1349', 'pixel_1350', 'pixel_1351', 'pixel_1352', 'pixel_1353', 'pixel_1354', 'pixel_1355', 'pixel_1356', 'pixel_1357', 'pixel_1358', 'pixel_1359', 'pixel_1360', 'pixel_1361', 'pixel_1362', 'pixel_1363', 'pixel_1364', 'pixel_1365', 'pixel_1366', 'pixel_1367', 'pixel_1368', 'pixel_1369', 'pixel_1370', 'pixel_1371', 'pixel_1372', 'pixel_1373', 'pixel_1374', 'pixel_1375', 'pixel_1376', 'pixel_1377', 'pixel_1378', 'pixel_1379', 'pixel_1380', 'pixel_1381', 'pixel_1382', 'pixel_1383', 'pixel_1384', 'pixel_1385', 'pixel_1386', 'pixel_1387', 'pixel_1388', 'pixel_1389', 'pixel_1390', 'pixel_1391', 'pixel_1392', 'pixel_1393', 'pixel_1394', 'pixel_1395', 'pixel_1396', 'pixel_1397', 'pixel_1398', 'pixel_1399', 'pixel_1400', 'pixel_1401', 'pixel_1402', 'pixel_1403', 'pixel_1404', 'pixel_1405', 'pixel_1406', 'pixel_1407', 'pixel_1408', 'pixel_1409', 'pixel_1410', 'pixel_1411', 'pixel_1412', 'pixel_1413', 'pixel_1414', 'pixel_1415', 'pixel_1416', 'pixel_1417', 'pixel_1418', 'pixel_1419', 'pixel_1420', 'pixel_1421', 'pixel_1422', 'pixel_1423', 'pixel_1424', 'pixel_1425', 'pixel_1426', 'pixel_1427', 'pixel_1428', 'pixel_1429', 'pixel_1430', 'pixel_1431', 'pixel_1432', 'pixel_1433', 'pixel_1434', 'pixel_1435', 'pixel_1436', 'pixel_1437', 'pixel_1438', 'pixel_1439', 'pixel_1440', 'pixel_1441', 'pixel_1442', 'pixel_1443', 'pixel_1444', 'pixel_1445', 'pixel_1446', 'pixel_1447', 'pixel_1448', 'pixel_1449', 'pixel_1450', 'pixel_1451', 'pixel_1452', 'pixel_1453', 'pixel_1454', 'pixel_1455', 'pixel_1456', 'pixel_1457', 'pixel_1458', 'pixel_1459', 'pixel_1460', 'pixel_1461', 'pixel_1462', 'pixel_1463', 'pixel_1464', 'pixel_1465', 'pixel_1466', 'pixel_1467', 'pixel_1468', 'pixel_1469', 'pixel_1470', 'pixel_1471', 'pixel_1472', 'pixel_1473', 'pixel_1474', 'pixel_1475', 'pixel_1476', 'pixel_1477', 'pixel_1478', 'pixel_1479', 'pixel_1480', 'pixel_1481', 'pixel_1482', 'pixel_1483', 'pixel_1484', 'pixel_1485', 'pixel_1486', 'pixel_1487', 'pixel_1488', 'pixel_1489', 'pixel_1490', 'pixel_1491', 'pixel_1492', 'pixel_1493', 'pixel_1494', 'pixel_1495', 'pixel_1496', 'pixel_1497', 'pixel_1498', 'pixel_1499', 'pixel_1500', 'pixel_1501', 'pixel_1502', 'pixel_1503', 'pixel_1504', 'pixel_1505', 'pixel_1506', 'pixel_1507', 'pixel_1508', 'pixel_1509', 'pixel_1510', 'pixel_1511', 'pixel_1512', 'pixel_1513', 'pixel_1514', 'pixel_1515', 'pixel_1516', 'pixel_1517', 'pixel_1518', 'pixel_1519', 'pixel_1520', 'pixel_1521', 'pixel_1522', 'pixel_1523', 'pixel_1524', 'pixel_1525', 'pixel_1526', 'pixel_1527', 'pixel_1528', 'pixel_1529', 'pixel_1530', 'pixel_1531', 'pixel_1532', 'pixel_1533', 'pixel_1534', 'pixel_1535', 'pixel_1536', 'pixel_1537', 'pixel_1538', 'pixel_1539', 'pixel_1540', 'pixel_1541', 'pixel_1542', 'pixel_1543', 'pixel_1544', 'pixel_1545', 'pixel_1546', 'pixel_1547', 'pixel_1548', 'pixel_1549', 'pixel_1550', 'pixel_1551', 'pixel_1552', 'pixel_1553', 'pixel_1554', 'pixel_1555', 'pixel_1556', 'pixel_1557', 'pixel_1558', 'pixel_1559', 'pixel_1560', 'pixel_1561', 'pixel_1562', 'pixel_1563', 'pixel_1564', 'pixel_1565', 'pixel_1566', 'pixel_1567', 'pixel_1568', 'pixel_1569', 'pixel_1570', 'pixel_1571', 'pixel_1572', 'pixel_1573', 'pixel_1574', 'pixel_1575', 'pixel_1576', 'pixel_1577', 'pixel_1578', 'pixel_1579', 'pixel_1580', 'pixel_1581', 'pixel_1582', 'pixel_1583', 'pixel_1584', 'pixel_1585', 'pixel_1586', 'pixel_1587', 'pixel_1588', 'pixel_1589', 'pixel_1590', 'pixel_1591', 'pixel_1592', 'pixel_1593', 'pixel_1594', 'pixel_1595', 'pixel_1596', 'pixel_1597', 'pixel_1598', 'pixel_1599', 'pixel_1600', 'pixel_1601', 'pixel_1602', 'pixel_1603', 'pixel_1604', 'pixel_1605', 'pixel_1606', 'pixel_1607', 'pixel_1608', 'pixel_1609', 'pixel_1610', 'pixel_1611', 'pixel_1612', 'pixel_1613', 'pixel_1614', 'pixel_1615', 'pixel_1616', 'pixel_1617', 'pixel_1618', 'pixel_1619', 'pixel_1620', 'pixel_1621', 'pixel_1622', 'pixel_1623', 'pixel_1624', 'pixel_1625', 'pixel_1626', 'pixel_1627', 'pixel_1628', 'pixel_1629', 'pixel_1630', 'pixel_1631', 'pixel_1632', 'pixel_1633', 'pixel_1634', 'pixel_1635', 'pixel_1636', 'pixel_1637', 'pixel_1638', 'pixel_1639', 'pixel_1640', 'pixel_1641', 'pixel_1642', 'pixel_1643', 'pixel_1644', 'pixel_1645', 'pixel_1646', 'pixel_1647', 'pixel_1648', 'pixel_1649', 'pixel_1650', 'pixel_1651', 'pixel_1652', 'pixel_1653', 'pixel_1654', 'pixel_1655', 'pixel_1656', 'pixel_1657', 'pixel_1658', 'pixel_1659', 'pixel_1660', 'pixel_1661', 'pixel_1662', 'pixel_1663', 'pixel_1664', 'pixel_1665', 'pixel_1666', 'pixel_1667', 'pixel_1668', 'pixel_1669', 'pixel_1670', 'pixel_1671', 'pixel_1672', 'pixel_1673', 'pixel_1674', 'pixel_1675', 'pixel_1676', 'pixel_1677', 'pixel_1678', 'pixel_1679', 'pixel_1680', 'pixel_1681', 'pixel_1682', 'pixel_1683', 'pixel_1684', 'pixel_1685', 'pixel_1686', 'pixel_1687', 'pixel_1688', 'pixel_1689', 'pixel_1690', 'pixel_1691', 'pixel_1692', 'pixel_1693', 'pixel_1694', 'pixel_1695', 'pixel_1696', 'pixel_1697', 'pixel_1698', 'pixel_1699', 'pixel_1700', 'pixel_1701', 'pixel_1702', 'pixel_1703', 'pixel_1704', 'pixel_1705', 'pixel_1706', 'pixel_1707', 'pixel_1708', 'pixel_1709', 'pixel_1710', 'pixel_1711', 'pixel_1712', 'pixel_1713', 'pixel_1714', 'pixel_1715', 'pixel_1716', 'pixel_1717', 'pixel_1718', 'pixel_1719', 'pixel_1720', 'pixel_1721', 'pixel_1722', 'pixel_1723', 'pixel_1724', 'pixel_1725', 'pixel_1726', 'pixel_1727', 'pixel_1728', 'pixel_1729', 'pixel_1730', 'pixel_1731', 'pixel_1732', 'pixel_1733', 'pixel_1734', 'pixel_1735', 'pixel_1736', 'pixel_1737', 'pixel_1738', 'pixel_1739', 'pixel_1740', 'pixel_1741', 'pixel_1742', 'pixel_1743', 'pixel_1744', 'pixel_1745', 'pixel_1746', 'pixel_1747', 'pixel_1748', 'pixel_1749', 'pixel_1750', 'pixel_1751', 'pixel_1752', 'pixel_1753', 'pixel_1754', 'pixel_1755', 'pixel_1756', 'pixel_1757', 'pixel_1758', 'pixel_1759', 'pixel_1760', 'pixel_1761', 'pixel_1762', 'pixel_1763', 'pixel_1764', 'pixel_1765', 'pixel_1766', 'pixel_1767', 'pixel_1768', 'pixel_1769', 'pixel_1770', 'pixel_1771', 'pixel_1772', 'pixel_1773', 'pixel_1774', 'pixel_1775', 'pixel_1776', 'pixel_1777', 'pixel_1778', 'pixel_1779', 'pixel_1780', 'pixel_1781', 'pixel_1782', 'pixel_1783', 'pixel_1784', 'pixel_1785', 'pixel_1786', 'pixel_1787', 'pixel_1788', 'pixel_1789', 'pixel_1790', 'pixel_1791', 'pixel_1792', 'pixel_1793', 'pixel_1794', 'pixel_1795', 'pixel_1796', 'pixel_1797', 'pixel_1798', 'pixel_1799', 'pixel_1800', 'pixel_1801', 'pixel_1802', 'pixel_1803', 'pixel_1804', 'pixel_1805', 'pixel_1806', 'pixel_1807', 'pixel_1808', 'pixel_1809', 'pixel_1810', 'pixel_1811', 'pixel_1812', 'pixel_1813', 'pixel_1814', 'pixel_1815', 'pixel_1816', 'pixel_1817', 'pixel_1818', 'pixel_1819', 'pixel_1820', 'pixel_1821', 'pixel_1822', 'pixel_1823', 'pixel_1824', 'pixel_1825', 'pixel_1826', 'pixel_1827', 'pixel_1828', 'pixel_1829', 'pixel_1830', 'pixel_1831', 'pixel_1832', 'pixel_1833', 'pixel_1834', 'pixel_1835', 'pixel_1836', 'pixel_1837', 'pixel_1838', 'pixel_1839', 'pixel_1840', 'pixel_1841', 'pixel_1842', 'pixel_1843', 'pixel_1844', 'pixel_1845', 'pixel_1846', 'pixel_1847', 'pixel_1848', 'pixel_1849', 'pixel_1850', 'pixel_1851', 'pixel_1852', 'pixel_1853', 'pixel_1854', 'pixel_1855', 'pixel_1856', 'pixel_1857', 'pixel_1858', 'pixel_1859', 'pixel_1860', 'pixel_1861', 'pixel_1862', 'pixel_1863', 'pixel_1864', 'pixel_1865', 'pixel_1866', 'pixel_1867', 'pixel_1868', 'pixel_1869', 'pixel_1870', 'pixel_1871', 'pixel_1872', 'pixel_1873', 'pixel_1874', 'pixel_1875', 'pixel_1876', 'pixel_1877', 'pixel_1878', 'pixel_1879', 'pixel_1880', 'pixel_1881', 'pixel_1882', 'pixel_1883', 'pixel_1884', 'pixel_1885', 'pixel_1886', 'pixel_1887', 'pixel_1888', 'pixel_1889', 'pixel_1890', 'pixel_1891', 'pixel_1892', 'pixel_1893', 'pixel_1894', 'pixel_1895', 'pixel_1896', 'pixel_1897', 'pixel_1898', 'pixel_1899', 'pixel_1900', 'pixel_1901', 'pixel_1902', 'pixel_1903', 'pixel_1904', 'pixel_1905', 'pixel_1906', 'pixel_1907', 'pixel_1908', 'pixel_1909', 'pixel_1910', 'pixel_1911', 'pixel_1912', 'pixel_1913', 'pixel_1914', 'pixel_1915', 'pixel_1916', 'pixel_1917', 'pixel_1918', 'pixel_1919', 'pixel_1920', 'pixel_1921', 'pixel_1922', 'pixel_1923', 'pixel_1924', 'pixel_1925', 'pixel_1926', 'pixel_1927', 'pixel_1928', 'pixel_1929', 'pixel_1930', 'pixel_1931', 'pixel_1932', 'pixel_1933', 'pixel_1934', 'pixel_1935', 'pixel_1936', 'pixel_1937', 'pixel_1938', 'pixel_1939', 'pixel_1940', 'pixel_1941', 'pixel_1942', 'pixel_1943', 'pixel_1944', 'pixel_1945', 'pixel_1946', 'pixel_1947', 'pixel_1948', 'pixel_1949', 'pixel_1950', 'pixel_1951', 'pixel_1952', 'pixel_1953', 'pixel_1954', 'pixel_1955', 'pixel_1956', 'pixel_1957', 'pixel_1958', 'pixel_1959', 'pixel_1960', 'pixel_1961', 'pixel_1962', 'pixel_1963', 'pixel_1964', 'pixel_1965', 'pixel_1966', 'pixel_1967', 'pixel_1968', 'pixel_1969', 'pixel_1970', 'pixel_1971', 'pixel_1972', 'pixel_1973', 'pixel_1974', 'pixel_1975', 'pixel_1976', 'pixel_1977', 'pixel_1978', 'pixel_1979', 'pixel_1980', 'pixel_1981', 'pixel_1982', 'pixel_1983', 'pixel_1984', 'pixel_1985', 'pixel_1986', 'pixel_1987', 'pixel_1988', 'pixel_1989', 'pixel_1990', 'pixel_1991', 'pixel_1992', 'pixel_1993', 'pixel_1994', 'pixel_1995', 'pixel_1996', 'pixel_1997', 'pixel_1998', 'pixel_1999', 'pixel_2000', 'pixel_2001', 'pixel_2002', 'pixel_2003', 'pixel_2004', 'pixel_2005', 'pixel_2006', 'pixel_2007', 'pixel_2008', 'pixel_2009', 'pixel_2010', 'pixel_2011', 'pixel_2012', 'pixel_2013', 'pixel_2014', 'pixel_2015', 'pixel_2016', 'pixel_2017', 'pixel_2018', 'pixel_2019', 'pixel_2020', 'pixel_2021', 'pixel_2022', 'pixel_2023', 'pixel_2024', 'pixel_2025', 'pixel_2026', 'pixel_2027', 'pixel_2028', 'pixel_2029', 'pixel_2030', 'pixel_2031', 'pixel_2032', 'pixel_2033', 'pixel_2034', 'pixel_2035', 'pixel_2036', 'pixel_2037', 'pixel_2038', 'pixel_2039', 'pixel_2040', 'pixel_2041', 'pixel_2042', 'pixel_2043', 'pixel_2044', 'pixel_2045', 'pixel_2046', 'pixel_2047', 'pixel_2048', 'pixel_2049', 'pixel_2050', 'pixel_2051', 'pixel_2052', 'pixel_2053', 'pixel_2054', 'pixel_2055', 'pixel_2056', 'pixel_2057', 'pixel_2058', 'pixel_2059', 'pixel_2060', 'pixel_2061', 'pixel_2062', 'pixel_2063', 'pixel_2064', 'pixel_2065', 'pixel_2066', 'pixel_2067', 'pixel_2068', 'pixel_2069', 'pixel_2070', 'pixel_2071', 'pixel_2072', 'pixel_2073', 'pixel_2074', 'pixel_2075', 'pixel_2076', 'pixel_2077', 'pixel_2078', 'pixel_2079', 'pixel_2080', 'pixel_2081', 'pixel_2082', 'pixel_2083', 'pixel_2084', 'pixel_2085', 'pixel_2086', 'pixel_2087', 'pixel_2088', 'pixel_2089', 'pixel_2090', 'pixel_2091', 'pixel_2092', 'pixel_2093', 'pixel_2094', 'pixel_2095', 'pixel_2096', 'pixel_2097', 'pixel_2098', 'pixel_2099', 'pixel_2100', 'pixel_2101', 'pixel_2102', 'pixel_2103', 'pixel_2104', 'pixel_2105', 'pixel_2106', 'pixel_2107', 'pixel_2108', 'pixel_2109', 'pixel_2110', 'pixel_2111', 'pixel_2112', 'pixel_2113', 'pixel_2114', 'pixel_2115', 'pixel_2116', 'pixel_2117', 'pixel_2118', 'pixel_2119', 'pixel_2120', 'pixel_2121', 'pixel_2122', 'pixel_2123', 'pixel_2124', 'pixel_2125', 'pixel_2126', 'pixel_2127', 'pixel_2128', 'pixel_2129', 'pixel_2130', 'pixel_2131', 'pixel_2132', 'pixel_2133', 'pixel_2134', 'pixel_2135', 'pixel_2136', 'pixel_2137', 'pixel_2138', 'pixel_2139', 'pixel_2140', 'pixel_2141', 'pixel_2142', 'pixel_2143', 'pixel_2144', 'pixel_2145', 'pixel_2146', 'pixel_2147', 'pixel_2148', 'pixel_2149', 'pixel_2150', 'pixel_2151', 'pixel_2152', 'pixel_2153', 'pixel_2154', 'pixel_2155', 'pixel_2156', 'pixel_2157', 'pixel_2158', 'pixel_2159', 'pixel_2160', 'pixel_2161', 'pixel_2162', 'pixel_2163', 'pixel_2164', 'pixel_2165', 'pixel_2166', 'pixel_2167', 'pixel_2168', 'pixel_2169', 'pixel_2170', 'pixel_2171', 'pixel_2172', 'pixel_2173', 'pixel_2174', 'pixel_2175', 'pixel_2176', 'pixel_2177', 'pixel_2178', 'pixel_2179', 'pixel_2180', 'pixel_2181', 'pixel_2182', 'pixel_2183', 'pixel_2184', 'pixel_2185', 'pixel_2186', 'pixel_2187', 'pixel_2188', 'pixel_2189', 'pixel_2190', 'pixel_2191', 'pixel_2192', 'pixel_2193', 'pixel_2194', 'pixel_2195', 'pixel_2196', 'pixel_2197', 'pixel_2198', 'pixel_2199', 'pixel_2200', 'pixel_2201', 'pixel_2202', 'pixel_2203', 'pixel_2204', 'pixel_2205', 'pixel_2206', 'pixel_2207', 'pixel_2208', 'pixel_2209', 'pixel_2210', 'pixel_2211', 'pixel_2212', 'pixel_2213', 'pixel_2214', 'pixel_2215', 'pixel_2216', 'pixel_2217', 'pixel_2218', 'pixel_2219', 'pixel_2220', 'pixel_2221', 'pixel_2222', 'pixel_2223', 'pixel_2224', 'pixel_2225', 'pixel_2226', 'pixel_2227', 'pixel_2228', 'pixel_2229', 'pixel_2230', 'pixel_2231', 'pixel_2232', 'pixel_2233', 'pixel_2234', 'pixel_2235', 'pixel_2236', 'pixel_2237', 'pixel_2238', 'pixel_2239', 'pixel_2240', 'pixel_2241', 'pixel_2242', 'pixel_2243', 'pixel_2244', 'pixel_2245', 'pixel_2246', 'pixel_2247', 'pixel_2248', 'pixel_2249', 'pixel_2250', 'pixel_2251', 'pixel_2252', 'pixel_2253', 'pixel_2254', 'pixel_2255', 'pixel_2256', 'pixel_2257', 'pixel_2258', 'pixel_2259', 'pixel_2260', 'pixel_2261', 'pixel_2262', 'pixel_2263', 'pixel_2264', 'pixel_2265', 'pixel_2266', 'pixel_2267', 'pixel_2268', 'pixel_2269', 'pixel_2270', 'pixel_2271', 'pixel_2272', 'pixel_2273', 'pixel_2274', 'pixel_2275', 'pixel_2276', 'pixel_2277', 'pixel_2278', 'pixel_2279', 'pixel_2280', 'pixel_2281', 'pixel_2282', 'pixel_2283', 'pixel_2284', 'pixel_2285', 'pixel_2286', 'pixel_2287', 'pixel_2288', 'pixel_2289', 'pixel_2290', 'pixel_2291', 'pixel_2292', 'pixel_2293', 'pixel_2294', 'pixel_2295', 'pixel_2296', 'pixel_2297', 'pixel_2298', 'pixel_2299', 'pixel_2300', 'pixel_2301', 'pixel_2302', 'pixel_2303', 'label_emo']\n"
     ]
    }
   ],
   "source": [
    "list_name_col = []\n",
    "for i in range(train_pixel.shape[1]):\n",
    "    list_name_col.append('pixel_' + str(i))\n",
    "list_name_col.append('label_emo')\n",
    "print(list_name_col)\n",
    "df = pd.DataFrame(result_train, columns=list_name_col)\n",
    "df.to_csv(path_to, sep=',', index_label='serial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcf2bd74",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>pixel_0</th>\n",
       "      <th>pixel_1</th>\n",
       "      <th>pixel_2</th>\n",
       "      <th>pixel_3</th>\n",
       "      <th>pixel_4</th>\n",
       "      <th>pixel_5</th>\n",
       "      <th>pixel_6</th>\n",
       "      <th>pixel_7</th>\n",
       "      <th>pixel_8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_2295</th>\n",
       "      <th>pixel_2296</th>\n",
       "      <th>pixel_2297</th>\n",
       "      <th>pixel_2298</th>\n",
       "      <th>pixel_2299</th>\n",
       "      <th>pixel_2300</th>\n",
       "      <th>pixel_2301</th>\n",
       "      <th>pixel_2302</th>\n",
       "      <th>pixel_2303</th>\n",
       "      <th>label_emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>72</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>136</td>\n",
       "      <td>106</td>\n",
       "      <td>116</td>\n",
       "      <td>95</td>\n",
       "      <td>106</td>\n",
       "      <td>109</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>147</td>\n",
       "      <td>155</td>\n",
       "      <td>148</td>\n",
       "      <td>133</td>\n",
       "      <td>111</td>\n",
       "      <td>140</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>95</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "      <td>67</td>\n",
       "      <td>171</td>\n",
       "      <td>193</td>\n",
       "      <td>183</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>212</td>\n",
       "      <td>156</td>\n",
       "      <td>164</td>\n",
       "      <td>174</td>\n",
       "      <td>138</td>\n",
       "      <td>161</td>\n",
       "      <td>173</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>152</td>\n",
       "      <td>122</td>\n",
       "      <td>114</td>\n",
       "      <td>101</td>\n",
       "      <td>97</td>\n",
       "      <td>88</td>\n",
       "      <td>110</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>126</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>136</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28704</th>\n",
       "      <td>28704</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>125</td>\n",
       "      <td>231</td>\n",
       "      <td>215</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28705</th>\n",
       "      <td>28705</td>\n",
       "      <td>114</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>107</td>\n",
       "      <td>120</td>\n",
       "      <td>141</td>\n",
       "      <td>144</td>\n",
       "      <td>132</td>\n",
       "      <td>129</td>\n",
       "      <td>182</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28706</th>\n",
       "      <td>28706</td>\n",
       "      <td>74</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>89</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>192</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28707</th>\n",
       "      <td>28707</td>\n",
       "      <td>222</td>\n",
       "      <td>227</td>\n",
       "      <td>203</td>\n",
       "      <td>90</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>77</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>139</td>\n",
       "      <td>141</td>\n",
       "      <td>145</td>\n",
       "      <td>137</td>\n",
       "      <td>139</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28708</th>\n",
       "      <td>28708</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>205</td>\n",
       "      <td>206</td>\n",
       "      <td>205</td>\n",
       "      <td>203</td>\n",
       "      <td>206</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>67</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28709 rows × 2306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       serial  pixel_0  pixel_1  pixel_2  pixel_3  pixel_4  pixel_5  pixel_6  \\\n",
       "0           0       70       80       82       72       58       58       60   \n",
       "1           1      151      150      147      155      148      133      111   \n",
       "2           2      231      212      156      164      174      138      161   \n",
       "3           3       24       32       36       30       32       23       19   \n",
       "4           4        4        0        0        0        0        0        0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "28704   28704       84       85       85       85       85       85       85   \n",
       "28705   28705      114      112      113      113      111      111      112   \n",
       "28706   28706       74       81       87       89       95      100       98   \n",
       "28707   28707      222      227      203       90       86       90       84   \n",
       "28708   28708      195      199      205      206      205      203      206   \n",
       "\n",
       "       pixel_7  pixel_8  ...  pixel_2295  pixel_2296  pixel_2297  pixel_2298  \\\n",
       "0           63       54  ...         182         183         136         106   \n",
       "1          140      170  ...         108          95         108         102   \n",
       "2          173      182  ...         138         152         122         114   \n",
       "3           20       30  ...         126         132         132         133   \n",
       "4            0        0  ...          34          31          31          31   \n",
       "...        ...      ...  ...         ...         ...         ...         ...   \n",
       "28704       85       86  ...          34          35          36          40   \n",
       "28705      113      115  ...          94         107         120         141   \n",
       "28706       93      105  ...         214         211         209         200   \n",
       "28707       77       94  ...         139         141         145         137   \n",
       "28708      209      208  ...          93          67          34           6   \n",
       "\n",
       "       pixel_2299  pixel_2300  pixel_2301  pixel_2302  pixel_2303  label_emo  \n",
       "0             116          95         106         109          82          0  \n",
       "1              67         171         193         183         184          0  \n",
       "2             101          97          88         110         152          2  \n",
       "3             136         139         142         143         142          4  \n",
       "4              27          31          30          29          30          6  \n",
       "...           ...         ...         ...         ...         ...        ...  \n",
       "28704          27         125         231         215         200          2  \n",
       "28705         144         132         129         182         222          0  \n",
       "28706         195         192         188         187         187          4  \n",
       "28707         139         136         136         136         134          0  \n",
       "28708           7           3           6          15          38          4  \n",
       "\n",
       "[28709 rows x 2306 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(path_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c53524",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# generate feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98c2e844",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "% matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms, models\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fea1783",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fer import FER2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "266a8781",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(44),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_train_new = transforms.Compose([\n",
    "    transforms.RandomCrop(44),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "daa11cc4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainset = FER2013(split='Training', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "PrivateTestset = FER2013(split='PrivateTest', transform=transform_train)\n",
    "PrivateTestloader = torch.utils.data.DataLoader(PrivateTestset, batch_size=64, shuffle=True)\n",
    "\n",
    "PublicTestset = FER2013(split='PublicTest', transform=transform_train)\n",
    "PublicTestloader = torch.utils.data.DataLoader(PublicTestset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b2e024e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainset = FER2013(split='Training', transform=transform_train_new)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "PrivateTestset = FER2013(split='PrivateTest', transform=transform_train_new)\n",
    "PrivateTestloader = torch.utils.data.DataLoader(PrivateTestset, batch_size=64, shuffle=True)\n",
    "\n",
    "PublicTestset = FER2013(split='PublicTest', transform=transform_train_new)\n",
    "PublicTestloader = torch.utils.data.DataLoader(PublicTestset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "51a88f27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ResNet model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Remove last softmax layer for deep feature extraction\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67753f3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "generate data for training containing private test for memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b69d82ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flat_data_arr = []  #input array\n",
    "target_arr = []  #output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "030a26dd",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate deep features from ResNet model by forward pass\n",
    "def image_feature_loader(dataloader):\n",
    "    for inputs, labels in dataloader:\n",
    "        logps = model.forward(inputs.to(device))\n",
    "        for i in range(len(labels)):\n",
    "            flat_data_arr.append(np.array(logps[:, :, 0, 0][i].cpu().detach()))\n",
    "            target_arr.append(np.array(labels[i].cpu().detach()))\n",
    "\n",
    "\n",
    "image_feature_loader(trainloader)\n",
    "image_feature_loader(PrivateTestloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b63378b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data into dataframe and save \n",
    "flat_data = np.array(flat_data_arr)\n",
    "target = np.array(target_arr)\n",
    "df = pd.DataFrame(flat_data)  #dataframe\n",
    "df['Target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "df6d15a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save df to csv\n",
    "df.to_csv('deepfeature_train_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "316e4895",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save df to csv\n",
    "df.to_csv('deepfeature_train_all_withoutflip.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dc765",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "generate feature for public test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f84bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flat_data_arr_test = []  #input array\n",
    "target_arr_test = []  #output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804413eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate deep features from ResNet model by forward pass\n",
    "def image_feature_loader_test(dataloader):\n",
    "    for inputs, labels in dataloader:\n",
    "        logps = model.forward(inputs.to(device))\n",
    "        print(inputs, labels)\n",
    "        for i in range(len(labels)):\n",
    "            flat_data_arr_test.append(np.array(logps[:, :, 0, 0][i].cpu().detach()))\n",
    "            target_arr_test.append(np.array(labels[i].cpu().detach()))\n",
    "\n",
    "\n",
    "image_feature_loader_test(PublicTestloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec627cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flat_data_test = np.array(flat_data_arr_test)\n",
    "target_test = np.array(target_arr_test)\n",
    "df_test = pd.DataFrame(flat_data_test)  #dataframe\n",
    "df_test['Target'] = target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73396b17",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save df to csv\n",
    "df_test.to_csv('deepfeature_public_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df5c2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "generate feature for private test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29e091",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flat_data_arr_pri = []  #input array\n",
    "target_arr_pri = []  #output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be0e00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate deep features from ResNet model by forward pass\n",
    "def image_feature_loader_pri(dataloader):\n",
    "    for inputs, labels in dataloader:\n",
    "        logps = model.forward(inputs.to(device))\n",
    "        print(inputs, labels)\n",
    "        for i in range(len(labels)):\n",
    "            flat_data_arr_pri.append(np.array(logps[:, :, 0, 0][i].cpu().detach()))\n",
    "            target_arr_pri.append(np.array(labels[i].cpu().detach()))\n",
    "\n",
    "\n",
    "image_feature_loader_pri(PrivateTestloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023db6a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flat_data_pri = np.array(flat_data_arr_pri)\n",
    "target_pri = np.array(target_arr_pri)\n",
    "df_pri = pd.DataFrame(flat_data_pri)  #dataframe\n",
    "df_pri['Target'] = target_pri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb86b80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save df to csv\n",
    "df_test.to_csv('deepfeature_private_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "run \"brainome deepfeature_train_all.csv\" to get the result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "20a901ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training for memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb0239ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "epochs = 250\n",
    "input_size = 2048\n",
    "num_classes = 7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91525d51",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetFromCSV(data.Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.labels = np.asarray(self.data.iloc[:, -1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_feature = np.asarray(self.data.iloc[index][:-1])\n",
    "        single_img_label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            img_feature = self.transform(img_feature)\n",
    "        return (img_feature, single_img_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2eb2985",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform_train_feature = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eed99637",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====End loading training data=====\n"
     ]
    }
   ],
   "source": [
    "traindata = DatasetFromCSV('deepfeature_2class.csv', transform=None)\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"====End loading training data=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "810e851f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size / 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_size / 2, input_size / 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, num_classes),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "268734c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dafb9f2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    global Train_acc\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(torch.float32), targets.to(torch.float32)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        #         print(type(inputs[0]), type(targets))\n",
    "\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets.long())\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        utils.progress_bar(batch_idx, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                           % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    Train_acc = 100. * correct / total\n",
    "    print(\"=====train_acc======\", Train_acc, \"=====epoch=====\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5a588c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 1.719 | Acc: 30.961% (8897/28736)     449/449 449 .........................] | Loss: 1.756 | Acc: 29.818% (229/768)        12/449 13/449  40/449 ....................] | Loss: 1.724 | Acc: 31.818% (1344/4224)      66/449 ] | Loss: 1.724 | Acc: 31.785% (1485/4672)      73/44 86/449 ........] | Loss: 1.724 | Acc: 31.645% (1762/5568)      87/44 90/449 ..] | Loss: 1.716 | Acc: 31.551% (2201/6976)      109/449 ............] | Loss: 1.713 | Acc: 31.436% (3038/9664)      151/449 ................] | Loss: 1.718 | Acc: 31.080% (3481/11200)     175/449 =======>..................] | Loss: 1.718 | Acc: 31.072% (3500/11264)     176/44 178/449 180/449 ======>.................] | Loss: 1.720 | Acc: 31.040% (3695/11904)     186/449 ============>.................] | Loss: 1.720 | Acc: 31.058% (3717/11968)     187/449 ......] | Loss: 1.720 | Acc: 30.909% (4075/13184)     206/449 ....] | Loss: 1.720 | Acc: 30.960% (4161/13440)     210/449 211/44 220/449 240/449 241/44 247/449 ...........] | Loss: 1.718 | Acc: 31.004% (5040/16256)     254/44 268/449 ==============>..........] | Loss: 1.715 | Acc: 31.261% (5942/19008)     297/449 =========>.........] | Loss: 1.716 | Acc: 31.184% (6147/19712)     308/449 =========>.........] | Loss: 1.716 | Acc: 31.164% (6163/19776)     309/44 317/449 ....] | Loss: 1.715 | Acc: 31.172% (6364/20416)     319/449 ==========>........] | Loss: 1.716 | Acc: 31.144% (6478/20800)     325/449 =======================>......] | Loss: 1.718 | Acc: 31.086% (7023/22592)     353/449 ===========>.....] | Loss: 1.718 | Acc: 31.078% (7220/23232)     363/449 374/44 375/449 =======>....] | Loss: 1.718 | Acc: 31.023% (7505/24192)     378/44 386/449 ======>...] | Loss: 1.719 | Acc: 30.932% (7780/25152)     393/449 ============>...] | Loss: 1.719 | Acc: 30.961% (7807/25216)     394/449 =============>...] | Loss: 1.719 | Acc: 30.990% (7854/25344)     396/449 397/449 =========>...] | Loss: 1.718 | Acc: 31.114% (7985/25664)     401/449 403/449 ==>...] | Loss: 1.719 | Acc: 31.060% (8031/25856)     404/449 ==========>..] | Loss: 1.719 | Acc: 31.041% (8304/26752)     418/449 ==========>.] | Loss: 1.719 | Acc: 31.020% (8358/26944)     421/449 ==========>.] | Loss: 1.719 | Acc: 30.987% (8488/27392)     428/449 ========>.] | Loss: 1.719 | Acc: 31.006% (8513/27456)     429/449 ===================>.] | Loss: 1.720 | Acc: 30.948% (8616/27840)     435/449     441/449 =========================>] | Loss: 1.720 | Acc: 30.950% (8755/28288)     442/449   446/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.962% (8917/28800)     450/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.983% (8943/28864)     451/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 31.001% (8968/28928)     452/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.998% (8987/28992)     453/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.995% (9006/29056)     454/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 31.006% (9029/29120)     455/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.976% (9040/29184)     456/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.983% (9062/29248)     457/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.987% (9083/29312)     458/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.974% (9099/29376)   459/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.965% (9116/29440)     460/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 30.969% (9137/29504)     461/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 30.986% (9162/29568)     462/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 31.000% (9186/29632)     463/449 \n",
      " [==============================>] | Loss: 1.719 | Acc: 31.038% (9217/29696)     464/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.032% (9235/29760)     465/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.015% (9250/29824)     466/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.022% (9272/29888)     467/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.033% (9295/29952)   468/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.047% (9319/30016)     469/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.037% (9336/30080)     470/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.021% (9351/30144)     471/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.032% (9374/30208)     472/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 31.025% (9392/30272)     473/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.019% (9410/30336)   474/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 31.013% (9428/30400)     475/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.046% (9458/30464)     476/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.034% (9474/30528)     477/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.034% (9494/30592)     478/449 \n",
      " [===============================>] | Loss: 1.719 | Acc: 31.041% (9516/30656)     479/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.035% (9534/30720)     480/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.032% (9553/30784)     481/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.039% (9575/30848)     482/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.040% (9595/30912)     483/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.063% (9622/30976)     484/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.070% (9644/31040)     485/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.054% (9659/31104)   486/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.054% (9679/31168)     487/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.061% (9701/31232)     488/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.077% (9726/31296)     489/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.075% (9745/31360)     490/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.072% (9764/31424)     491/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.079% (9786/31488)     492/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.079% (9806/31552)   493/449 \n",
      " [================================>] | Loss: 1.719 | Acc: 31.095% (9831/31616)     494/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.095% (9851/31680)     495/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.108% (9875/31744)     496/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.090% (9889/31808)     497/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.103% (9913/31872)   498/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.118% (9938/31936)   499/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.119% (9958/32000)     500/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.141% (9985/32064)     501/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.144% (10006/32128)    502/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.144% (10026/32192)    503/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.123% (10039/32256)    504/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.120% (10058/32320)    505/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.142% (10085/32384)    506/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.127% (10100/32448)    507/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.136% (10123/32512)    508/449 \n",
      " [=================================>] | Loss: 1.719 | Acc: 31.127% (10140/32576)    509/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.137% (10163/32640)  510/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.134% (10182/32704)    511/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.137% (10203/32768)    512/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================>] | Loss: 1.720 | Acc: 31.119% (10217/32832)    513/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.131% (10241/32896)    514/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.132% (10261/32960)    515/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.132% (10281/33024)    516/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.147% (10306/33088)    517/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.178% (10336/33152)    518/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.172% (10354/33216)    519/449 \n",
      " [==================================>] | Loss: 1.719 | Acc: 31.190% (10380/33280)    520/449 \n",
      " [==================================>] | Loss: 1.718 | Acc: 31.214% (10408/33344)    521/449 \n",
      " [==================================>] | Loss: 1.718 | Acc: 31.202% (10424/33408)    522/449 \n",
      " [==================================>] | Loss: 1.718 | Acc: 31.208% (10446/33472)    523/449 \n",
      " [==================================>] | Loss: 1.718 | Acc: 31.223% (10471/33536)    524/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.223% (10491/33600)    525/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.223% (10511/33664)    526/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.214% (10528/33728)    527/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.212% (10547/33792)    528/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.215% (10568/33856)    529/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.209% (10586/33920)    530/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.221% (10610/33984)    531/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.212% (10627/34048)    532/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.215% (10648/34112)    533/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.212% (10667/34176)    534/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.235% (10695/34240)    535/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.235% (10715/34304)  536/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.241% (10737/34368)    537/449 \n",
      " [===================================>] | Loss: 1.718 | Acc: 31.227% (10752/34432)    538/449 \n",
      " [===================================>] | Loss: 1.719 | Acc: 31.224% (10771/34496)    539/449 \n",
      " [====================================>] | Loss: 1.719 | Acc: 31.236% (10795/34560)    540/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.264% (10825/34624)    541/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.273% (10848/34688)  542/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.282% (10871/34752)    543/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.282% (10891/34816)    544/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.273% (10908/34880)    545/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.256% (10922/34944)    546/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.261% (10944/35008)    547/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.264% (10965/35072)    548/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.256% (10982/35136)    549/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.250% (11000/35200)    550/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.250% (11020/35264)    551/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.264% (11045/35328)    552/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.270% (11067/35392)    553/449 \n",
      " [====================================>] | Loss: 1.718 | Acc: 31.267% (11086/35456)    554/449 \n",
      " [=====================================>] | Loss: 1.718 | Acc: 31.284% (11112/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.718 | Acc: 31.281% (11131/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.718 | Acc: 31.292% (11155/35648)    557/449 \n",
      " [=====================================>] | Loss: 1.718 | Acc: 31.270% (11167/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.718 | Acc: 31.272% (11188/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.292% (11215/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.289% (11234/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.300% (11258/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.317% (11284/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.316% (11304/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.319% (11325/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.308% (11341/36224)    566/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.316% (11364/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.313% (11383/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.717 | Acc: 31.299% (11398/36416)    569/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.286% (11413/36480)    570/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.255% (11422/36544)    571/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.247% (11439/36608)    572/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.228% (11452/36672)    573/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.234% (11474/36736)    574/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.226% (11491/36800)    575/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.220% (11509/36864)    576/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.220% (11529/36928)    577/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.215% (11547/36992)    578/449 \n",
      " [======================================>] | Loss: 1.718 | Acc: 31.204% (11563/37056)    579/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.207% (11584/37120)    580/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.199% (11601/37184)    581/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.196% (11620/37248)    582/449 \n",
      " [======================================>] | Loss: 1.717 | Acc: 31.194% (11639/37312)    583/449 \n",
      " [======================================>] | Loss: 1.718 | Acc: 31.194% (11659/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.191% (11678/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.189% (11697/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.173% (11711/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.718 | Acc: 31.178% (11733/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.165% (11748/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.149% (11762/37760)    590/449 \n",
      " [=======================================>] | Loss: 1.718 | Acc: 31.136% (11777/37824)    591/449 \n",
      " [=======================================>] | Loss: 1.718 | Acc: 31.126% (11793/37888)    592/449 \n",
      " [=======================================>] | Loss: 1.718 | Acc: 31.113% (11808/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.718 | Acc: 31.108% (11826/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.718 | Acc: 31.113% (11848/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.140% (11878/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.140% (11898/38208)    597/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=======================================>] | Loss: 1.717 | Acc: 31.145% (11920/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.717 | Acc: 31.146% (11940/38336)    599/449 \n",
      " [========================================>] | Loss: 1.717 | Acc: 31.133% (11955/38400)    600/449 \n",
      " [========================================>] | Loss: 1.717 | Acc: 31.123% (11971/38464)    601/449 \n",
      " [========================================>] | Loss: 1.717 | Acc: 31.125% (11992/38528)    602/449 \n",
      " [========================================>] | Loss: 1.717 | Acc: 31.102% (12003/38592)    603/449 \n",
      " [========================================>] | Loss: 1.717 | Acc: 31.082% (12015/38656)    604/449 \n",
      " [========================================>] | Loss: 1.717 | Acc: 31.069% (12030/38720)    605/449 \n",
      " [========================================>] | Loss: 1.718 | Acc: 31.064% (12048/38784)    606/449 \n",
      " [========================================>] | Loss: 1.718 | Acc: 31.052% (12063/38848)  607/449 \n",
      " [========================================>] | Loss: 1.718 | Acc: 31.026% (12073/38912)    608/449 \n",
      " [========================================>] | Loss: 1.719 | Acc: 31.009% (12086/38976)    609/449 \n",
      " [========================================>] | Loss: 1.719 | Acc: 31.009% (12106/39040)    610/449 \n",
      " [========================================>] | Loss: 1.719 | Acc: 30.997% (12121/39104)    611/449 \n",
      " [========================================>] | Loss: 1.719 | Acc: 31.002% (12143/39168)    612/449 \n",
      " [========================================>] | Loss: 1.719 | Acc: 31.023% (12171/39232)    613/449 \n",
      " [========================================>] | Loss: 1.719 | Acc: 31.034% (12195/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.039% (12217/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.045% (12239/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.035% (12255/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.020% (12269/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.720 | Acc: 31.013% (12286/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.720 | Acc: 31.008% (12304/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.024% (12330/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.044% (12358/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.042% (12377/39872)  623/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.050% (12400/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.050% (12420/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.048% (12439/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.053% (12461/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.053% (12481/40192)    628/449 \n",
      " [=========================================>] | Loss: 1.719 | Acc: 31.050% (12491/40229)    629/449 \n",
      "=====train_acc====== tensor(31.0497) =====epoch===== 0\n",
      " [=============================>] | Loss: 1.723 | Acc: 32.085% (9220/28736)     449/449 .....................] | Loss: 1.709 | Acc: 32.812% (189/576)        9/44 15/44 16/449 ..] | Loss: 1.718 | Acc: 32.812% (378/1152)       18/449 23/449    27/449 ........................] | Loss: 1.709 | Acc: 32.986% (760/2304)       36/449 37/449 38/449 ..........] | Loss: 1.716 | Acc: 32.254% (867/2688)       42/449 ......................] | Loss: 1.714 | Acc: 32.282% (1095/3392)      53/44 55/449 61/44 63/449 .................] | Loss: 1.718 | Acc: 31.592% (1294/4096)      64/449 .......................] | Loss: 1.717 | Acc: 31.795% (1343/4224)      66/449 67/449 .] | Loss: 1.717 | Acc: 31.652% (1418/4480)      70/449   77/449 81/449 ===>........................] | Loss: 1.718 | Acc: 31.965% (1698/5312)      83/449 ...................] | Loss: 1.718 | Acc: 32.132% (1748/5440)      85/449 ..................] | Loss: 1.717 | Acc: 32.177% (1771/5504)      86/449 ..........] | Loss: 1.722 | Acc: 31.904% (2001/6272)      98/449 99/449 ===>......................] | Loss: 1.728 | Acc: 31.552% (2403/7616)      119/449 =====>.....................] | Loss: 1.730 | Acc: 31.529% (2482/7872)      123/449 ==>.....................] | Loss: 1.730 | Acc: 31.452% (2496/7936)      124/449 ========>.....................] | Loss: 1.729 | Acc: 31.519% (2582/8192)      128/449 ========>.....................] | Loss: 1.728 | Acc: 31.530% (2704/8576)      134/449 ======>....................] | Loss: 1.730 | Acc: 31.473% (2820/8960)      140/449 =====>....................] | Loss: 1.729 | Acc: 31.627% (2935/9280)      145/44 146/449 ========>...................] | Loss: 1.729 | Acc: 31.784% (3214/10112)     158/449 ......] | Loss: 1.730 | Acc: 31.761% (3232/10176)     159/449 168/449 170/449 ===========>..................] | Loss: 1.730 | Acc: 31.503% (3488/11072)     173/449 177/449 ..........] | Loss: 1.730 | Acc: 31.641% (3807/12032)     188/44 189/449     191/449 .........] | Loss: 1.730 | Acc: 31.535% (3875/12288)     192/44 195/44 211/449 =>...............] | Loss: 1.726 | Acc: 31.666% (4418/13952)     218/449 ===========>...............] | Loss: 1.726 | Acc: 31.674% (4480/14144)     221/449 222/449 =========>...............] | Loss: 1.726 | Acc: 31.719% (4527/14272)     223/449 ....] | Loss: 1.726 | Acc: 31.676% (4541/14336)     224/449 =======>..............] | Loss: 1.726 | Acc: 31.727% (4589/14464)     226/449 ...........] | Loss: 1.727 | Acc: 31.704% (4606/14528)     227/449 ============>..............] | Loss: 1.726 | Acc: 31.730% (4691/14784)     231/449 =========>..............] | Loss: 1.727 | Acc: 31.646% (4719/14912)     233/44 234/44 246/449  | Loss: 1.725 | Acc: 31.678% (5028/15872)     248/449 ...] | Loss: 1.725 | Acc: 31.702% (5052/15936)     249/449 ........] | Loss: 1.725 | Acc: 31.785% (5289/16640)     260/449 >............] | Loss: 1.724 | Acc: 31.837% (5481/17216)     269/449 =============>..........] | Loss: 1.724 | Acc: 31.774% (5816/18304)     286/449 289/449 ===================>..........] | Loss: 1.725 | Acc: 31.803% (5923/18624)     291/449 ======>..........] | Loss: 1.724 | Acc: 31.965% (6035/18880)     295/449 ==================>.........] | Loss: 1.723 | Acc: 31.940% (6153/19264)     301/449 =================>.........] | Loss: 1.723 | Acc: 31.929% (6253/19584)     306/449 ================>.........] | Loss: 1.724 | Acc: 31.902% (6309/19776)     309/449 ==========>.........] | Loss: 1.723 | Acc: 31.974% (6405/20032)     313/44 314/449 ......] | Loss: 1.724 | Acc: 32.153% (7264/22592)     353/449 ====================>......] | Loss: 1.724 | Acc: 32.179% (7311/22720)     355/449 ==============>.....] | Loss: 1.724 | Acc: 32.180% (7476/23232)     363/449 365/449  387/449 ======================>....] | Loss: 1.724 | Acc: 32.167% (8029/24960)     390/449 =======>...] | Loss: 1.724 | Acc: 32.178% (8114/25216)     394/449 ======================>...] | Loss: 1.724 | Acc: 32.195% (8180/25408)     397/44 404/449 ========>..] | Loss: 1.723 | Acc: 32.127% (8348/25984)     406/449 >..] | Loss: 1.724 | Acc: 32.084% (8460/26368)     412/449 ================>..] | Loss: 1.723 | Acc: 32.084% (8501/26496)     414/449   421/449 428/449 =======================>] | Loss: 1.723 | Acc: 32.058% (9048/28224)     441/449 ===============>] | Loss: 1.724 | Acc: 32.044% (9126/28480)     445/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.076% (9238/28800)     450/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.068% (9256/28864)     451/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.069% (9277/28928)     452/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.067% (9297/28992)     453/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.062% (9316/29056)     454/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.043% (9331/29120)     455/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.072% (9360/29184)   456/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.088% (9385/29248)     457/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.093% (9407/29312)     458/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.101% (9430/29376)     459/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.086% (9446/29440)     460/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.091% (9468/29504)     461/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.096% (9490/29568)   462/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.080% (9506/29632)     463/449 \n",
      " [==============================>] | Loss: 1.723 | Acc: 32.068% (9523/29696)     464/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.097% (9552/29760)     465/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.102% (9574/29824)     466/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.096% (9593/29888)     467/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.108% (9617/29952)     468/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.116% (9640/30016)     469/449 \n",
      " [===============================>] | Loss: 1.722 | Acc: 32.124% (9663/30080)     470/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.103% (9677/30144)     471/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.094% (9695/30208)     472/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.096% (9716/30272)     473/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.091% (9735/30336)     474/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.092% (9756/30400)     475/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.097% (9778/30464)     476/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.092% (9797/30528)     477/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.074% (9812/30592)     478/449 \n",
      " [===============================>] | Loss: 1.723 | Acc: 32.079% (9834/30656)     479/449 \n",
      " [================================>] | Loss: 1.723 | Acc: 32.070% (9852/30720)     480/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.078% (9875/30784)     481/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.073% (9894/30848)     482/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.078% (9916/30912)     483/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.070% (9934/30976)     484/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.072% (9955/31040)     485/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.086% (9980/31104)     486/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.078% (9998/31168)     487/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.086% (10021/31232)  488/449 \n",
      " [================================>] | Loss: 1.725 | Acc: 32.071% (10037/31296)    489/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.085% (10062/31360)    490/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.100% (10087/31424)    491/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.130% (10117/31488)    492/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.137% (10140/31552)    493/449 \n",
      " [================================>] | Loss: 1.724 | Acc: 32.151% (10165/31616)    494/449 \n",
      " [=================================>] | Loss: 1.724 | Acc: 32.162% (10189/31680)    495/449 \n",
      " [=================================>] | Loss: 1.724 | Acc: 32.167% (10211/31744)    496/449 \n",
      " [=================================>] | Loss: 1.723 | Acc: 32.193% (10240/31808)    497/449 \n",
      " [=================================>] | Loss: 1.724 | Acc: 32.179% (10256/31872)    498/449 \n",
      " [=================================>] | Loss: 1.724 | Acc: 32.177% (10276/31936)    499/449 \n",
      " [=================================>] | Loss: 1.725 | Acc: 32.178% (10297/32000)    500/449 \n",
      " [=================================>] | Loss: 1.725 | Acc: 32.167% (10314/32064)    501/449 \n",
      " [=================================>] | Loss: 1.725 | Acc: 32.153% (10330/32128)    502/449 \n",
      " [=================================>] | Loss: 1.725 | Acc: 32.142% (10347/32192)    503/449 \n",
      " [=================================>] | Loss: 1.725 | Acc: 32.140% (10367/32256)  504/449 \n",
      " [=================================>] | Loss: 1.726 | Acc: 32.116% (10380/32320)    505/449 \n",
      " [=================================>] | Loss: 1.726 | Acc: 32.093% (10393/32384)    506/449 \n",
      " [=================================>] | Loss: 1.726 | Acc: 32.079% (10409/32448)    507/449 \n",
      " [=================================>] | Loss: 1.726 | Acc: 32.062% (10424/32512)    508/449 \n",
      " [=================================>] | Loss: 1.726 | Acc: 32.067% (10446/32576)    509/449 \n",
      " [==================================>] | Loss: 1.726 | Acc: 32.050% (10461/32640)    510/449 \n",
      " [==================================>] | Loss: 1.726 | Acc: 32.024% (10473/32704)    511/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 32.016% (10491/32768)    512/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.996% (10505/32832)    513/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.964% (10515/32896)    514/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.942% (10528/32960)    515/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.940% (10548/33024)    516/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.939% (10568/33088)    517/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.944% (10590/33152)    518/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.939% (10609/33216)    519/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.944% (10631/33280)    520/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.922% (10644/33344)    521/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.909% (10660/33408)    522/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.898% (10677/33472)    523/449 \n",
      " [==================================>] | Loss: 1.727 | Acc: 31.906% (10700/33536)    524/449 \n",
      " [===================================>] | Loss: 1.727 | Acc: 31.899% (10718/33600)    525/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.886% (10734/33664)  526/449 \n",
      " [===================================>] | Loss: 1.727 | Acc: 31.882% (10753/33728)    527/449 \n",
      " [===================================>] | Loss: 1.727 | Acc: 31.880% (10773/33792)    528/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.870% (10790/33856)    529/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.846% (10802/33920)    530/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.830% (10817/33984)    531/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.829% (10837/34048)    532/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.810% (10851/34112)  533/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.809% (10871/34176)    534/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.799% (10888/34240)    535/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.789% (10905/34304)    536/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.806% (10931/34368)    537/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.799% (10949/34432)    538/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.795% (10968/34496)    539/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.774% (10981/34560)    540/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.755% (10995/34624)    541/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.752% (11014/34688)    542/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.745% (11032/34752)    543/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.761% (11058/34816)    544/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [====================================>] | Loss: 1.729 | Acc: 31.766% (11080/34880)    545/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.745% (11093/34944)    546/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.747% (11114/35008)    547/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.752% (11136/35072)    548/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.734% (11150/35136)    549/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.716% (11164/35200)    550/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.712% (11183/35264)    551/449 \n",
      " [====================================>] | Loss: 1.730 | Acc: 31.677% (11191/35328)  552/449 \n",
      " [====================================>] | Loss: 1.731 | Acc: 31.660% (11205/35392)    553/449 \n",
      " [====================================>] | Loss: 1.731 | Acc: 31.650% (11222/35456)    554/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.627% (11234/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.618% (11251/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.606% (11267/35648)    557/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.597% (11284/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.613% (11310/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.602% (11326/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.593% (11343/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.586% (11361/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.577% (11378/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.731 | Acc: 31.574% (11397/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.560% (11412/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.562% (11433/36224)  566/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.553% (11450/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.558% (11472/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.555% (11491/36416)    569/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.535% (11504/36480)    570/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.518% (11518/36544)    571/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.526% (11541/36608)    572/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.517% (11558/36672)    573/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.509% (11575/36736)    574/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.527% (11602/36800)    575/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.529% (11623/36864)    576/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.529% (11643/36928)    577/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.531% (11664/36992)    578/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.512% (11677/37056)    579/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.514% (11698/37120)    580/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.522% (11721/37184)    581/449 \n",
      " [======================================>] | Loss: 1.732 | Acc: 31.535% (11746/37248)    582/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.521% (11761/37312)    583/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.518% (11780/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.525% (11803/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.519% (11821/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.519% (11841/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.532% (11866/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.534% (11887/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.552% (11914/37760)    590/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.546% (11932/37824)    591/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.543% (11951/37888)  592/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.548% (11973/37952)  593/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.558% (11997/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.562% (12019/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.559% (12038/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.575% (12064/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.574% (12084/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.568% (12102/38336)    599/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.568% (12122/38400)    600/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.554% (12137/38464)    601/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.559% (12159/38528)    602/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.545% (12174/38592)    603/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.537% (12191/38656)    604/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.524% (12206/38720)  605/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.528% (12228/38784)    606/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.528% (12248/38848)    607/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.517% (12264/38912)  608/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.507% (12280/38976)    609/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.504% (12299/39040)    610/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.511% (12322/39104)    611/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.505% (12340/39168)    612/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.500% (12358/39232)    613/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.494% (12376/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.479% (12390/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.478% (12410/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.480% (12431/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.483% (12452/39552)  618/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.485% (12473/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.472% (12488/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.484% (12513/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.511% (12544/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.523% (12569/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.525% (12590/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.522% (12609/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.522% (12629/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.509% (12644/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 31.519% (12668/40192)    628/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.512% (12677/40229)    629/449 \n",
      "=====train_acc====== tensor(31.5121) =====epoch===== 1\n",
      " [=============================>] | Loss: 1.729 | Acc: 31.835% (9148/28736)     449/449 .....................] | Loss: 1.782 | Acc: 29.688% (38/128)         2/449       5/449 ] | Loss: 1.728 | Acc: 31.771% (183/576)        9/449 33/449 ....] | Loss: 1.713 | Acc: 32.629% (710/2176)       34/449 ..........................] | Loss: 1.706 | Acc: 32.413% (892/2752)       43/449 .....] | Loss: 1.708 | Acc: 32.454% (997/3072)       48/449      57/449  62/449 68/449 ........] | Loss: 1.706 | Acc: 32.369% (1678/5184)      81/449 ==>........................] | Loss: 1.707 | Acc: 32.525% (1811/5568)      87/44 93/449 ........] | Loss: 1.711 | Acc: 32.858% (2166/6592)      103/449 ......................] | Loss: 1.712 | Acc: 32.926% (2318/7040)      110/449 ===>......................] | Loss: 1.713 | Acc: 32.799% (2351/7168)      112/44 113/44 119/449 .........] | Loss: 1.722 | Acc: 32.408% (2883/8896)      139/449 >....................] | Loss: 1.723 | Acc: 32.366% (3045/9408)      147/449 150/449 ==>..........] | Loss: 1.720 | Acc: 32.438% (6062/18688)     292/449 299/44 304/449 =======>.........] | Loss: 1.725 | Acc: 32.123% (6332/19712)     308/44 318/449 ....] | Loss: 1.725 | Acc: 32.122% (6558/20416)     319/449 =====================>........] | Loss: 1.725 | Acc: 32.077% (6590/20544)     321/449 =================>........] | Loss: 1.725 | Acc: 32.110% (6720/20928)     327/449 =============>........] | Loss: 1.725 | Acc: 32.112% (6741/20992)     328/44 330/44 335/449 ============>.......] | Loss: 1.725 | Acc: 32.040% (6931/21632)     338/449 345/449 =======================>......] | Loss: 1.725 | Acc: 32.004% (7087/22144)     346/449 ===============>......] | Loss: 1.726 | Acc: 31.988% (7104/22208)     347/449 ======>......] | Loss: 1.725 | Acc: 31.991% (7125/22272)     348/449 >......] | Loss: 1.725 | Acc: 32.062% (7182/22400)     350/449 ......] | Loss: 1.725 | Acc: 32.076% (7226/22528)     352/449 ==================>......] | Loss: 1.725 | Acc: 32.109% (7254/22592)     353/449 =========>......] | Loss: 1.726 | Acc: 32.067% (7265/22656)     354/449 ================>......] | Loss: 1.726 | Acc: 32.029% (7318/22848)     357/449 368/449 ===>....] | Loss: 1.728 | Acc: 31.855% (7788/24448)     382/449 ===================>....] | Loss: 1.728 | Acc: 31.862% (7810/24512)     383/449 ==>....] | Loss: 1.728 | Acc: 31.863% (7851/24640)     385/449 ==========================>...] | Loss: 1.727 | Acc: 31.944% (8055/25216)     394/449 401/449 =============>..] | Loss: 1.728 | Acc: 31.942% (8586/26880)     420/44 421/449 ============================>.] | Loss: 1.728 | Acc: 31.932% (8665/27136)     424/449 =============>.] | Loss: 1.729 | Acc: 31.912% (8721/27328)     427/449 =========>.] | Loss: 1.729 | Acc: 31.931% (8767/27456)     429/449 430/449 =============>.] | Loss: 1.729 | Acc: 31.932% (8808/27584)     431/449 439/449 =====>] | Loss: 1.729 | Acc: 31.865% (9014/28288)     442/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.826% (9166/28800)     450/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.804% (9180/28864)     451/449 \n",
      " [==============================>] | Loss: 1.730 | Acc: 31.786% (9195/28928)     452/449 \n",
      " [==============================>] | Loss: 1.730 | Acc: 31.785% (9215/28992)     453/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.797% (9239/29056)     454/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.813% (9264/29120)     455/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.802% (9281/29184)     456/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.807% (9303/29248)     457/449 \n",
      " [==============================>] | Loss: 1.730 | Acc: 31.806% (9323/29312)   458/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.808% (9344/29376)     459/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.800% (9362/29440)     460/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.792% (9380/29504)   461/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.795% (9401/29568)     462/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.797% (9422/29632)     463/449 \n",
      " [==============================>] | Loss: 1.729 | Acc: 31.769% (9434/29696)     464/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.761% (9452/29760)     465/449 \n",
      " [===============================>] | Loss: 1.730 | Acc: 31.756% (9471/29824)     466/449 \n",
      " [===============================>] | Loss: 1.730 | Acc: 31.755% (9491/29888)     467/449 \n",
      " [===============================>] | Loss: 1.730 | Acc: 31.751% (9510/29952)     468/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.753% (9531/30016)     469/449 \n",
      " [===============================>] | Loss: 1.730 | Acc: 31.739% (9547/30080)     470/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.751% (9571/30144)     471/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.747% (9590/30208)     472/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.759% (9614/30272)     473/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.774% (9639/30336)     474/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.753% (9653/30400)     475/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.759% (9675/30464)     476/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.777% (9701/30528)     477/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.802% (9729/30592)     478/449 \n",
      " [===============================>] | Loss: 1.729 | Acc: 31.795% (9747/30656)     479/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.800% (9769/30720)     480/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.773% (9781/30784)   481/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.762% (9798/30848)     482/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.774% (9822/30912)     483/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.786% (9846/30976)     484/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.772% (9862/31040)     485/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.774% (9883/31104)     486/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.767% (9901/31168)     487/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.769% (9922/31232)     488/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.764% (9941/31296)     489/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.760% (9960/31360)     490/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.769% (9983/31424)     491/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.758% (10000/31488)    492/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.757% (10020/31552)    493/449 \n",
      " [================================>] | Loss: 1.729 | Acc: 31.753% (10039/31616)    494/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.755% (10060/31680)    495/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.757% (10081/31744)    496/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.753% (10100/31808)    497/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.746% (10118/31872)    498/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.738% (10136/31936)    499/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.744% (10158/32000)    500/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.749% (10180/32064)    501/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.754% (10202/32128)    502/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.735% (10216/32192)    503/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.731% (10235/32256)    504/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=================================>] | Loss: 1.729 | Acc: 31.733% (10256/32320)    505/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.716% (10271/32384)    506/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.725% (10294/32448)    507/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.714% (10311/32512)    508/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.717% (10332/32576)    509/449 \n",
      " [==================================>] | Loss: 1.729 | Acc: 31.700% (10347/32640)    510/449 \n",
      " [==================================>] | Loss: 1.729 | Acc: 31.681% (10361/32704)    511/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.683% (10382/32768)    512/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.698% (10407/32832)    513/449 \n",
      " [==================================>] | Loss: 1.729 | Acc: 31.694% (10426/32896)    514/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.708% (10451/32960)    515/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.695% (10467/33024)    516/449 \n",
      " [==================================>] | Loss: 1.729 | Acc: 31.688% (10485/33088)    517/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.684% (10504/33152)    518/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.696% (10528/33216)    519/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.698% (10549/33280)    520/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.712% (10574/33344)    521/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.735% (10602/33408)    522/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.728% (10620/33472)    523/449 \n",
      " [==================================>] | Loss: 1.728 | Acc: 31.709% (10634/33536)    524/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.717% (10657/33600)    525/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.719% (10678/33664)    526/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.721% (10699/33728)    527/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.732% (10723/33792)    528/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.743% (10747/33856)    529/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.733% (10764/33920)    530/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.733% (10784/33984)    531/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.735% (10805/34048)    532/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.731% (10824/34112)    533/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.733% (10845/34176)    534/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.752% (10872/34240)    535/449 \n",
      " [===================================>] | Loss: 1.728 | Acc: 31.743% (10889/34304)    536/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.727% (10904/34368)    537/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.703% (10916/34432)  538/449 \n",
      " [===================================>] | Loss: 1.729 | Acc: 31.702% (10936/34496)    539/449 \n",
      " [====================================>] | Loss: 1.728 | Acc: 31.704% (10957/34560)    540/449 \n",
      " [====================================>] | Loss: 1.728 | Acc: 31.695% (10974/34624)    541/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.688% (10992/34688)    542/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.670% (11006/34752)    543/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.646% (11018/34816)    544/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.657% (11042/34880)    545/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.654% (11061/34944)    546/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.641% (11077/35008)    547/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.649% (11100/35072)    548/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.648% (11120/35136)    549/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.668% (11147/35200)  550/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.675% (11170/35264)    551/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.660% (11185/35328)    552/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.668% (11208/35392)    553/449 \n",
      " [====================================>] | Loss: 1.729 | Acc: 31.667% (11228/35456)    554/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.667% (11248/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.674% (11271/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.693% (11298/35648)    557/449 \n",
      " [=====================================>] | Loss: 1.728 | Acc: 31.706% (11323/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.728 | Acc: 31.725% (11350/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.728 | Acc: 31.730% (11372/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.728 | Acc: 31.729% (11392/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.728 | Acc: 31.731% (11413/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.728 | Acc: 31.713% (11427/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.699% (11442/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.692% (11460/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.705% (11485/36224)  566/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.713% (11508/36288)  567/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.709% (11527/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.729 | Acc: 31.706% (11546/36416)    569/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.705% (11566/36480)    570/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.699% (11584/36544)    571/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.709% (11608/36608)    572/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.686% (11620/36672)    573/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.683% (11639/36736)    574/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.690% (11662/36800)    575/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.679% (11678/36864)    576/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.683% (11700/36928)  577/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.680% (11719/36992)  578/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.690% (11743/37056)    579/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.695% (11765/37120)    580/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.699% (11787/37184)    581/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.698% (11807/37248)    582/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.690% (11824/37312)    583/449 \n",
      " [======================================>] | Loss: 1.729 | Acc: 31.705% (11850/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.729 | Acc: 31.696% (11867/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.729 | Acc: 31.698% (11888/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.729 | Acc: 31.705% (11911/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.712% (11934/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.738% (11964/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.732% (11982/37760)  590/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.734% (12003/37824)  591/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.733% (12023/37888)    592/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.722% (12039/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.713% (12056/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.723% (12080/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.725% (12101/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.726% (12122/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.727 | Acc: 31.733% (12145/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 31.720% (12160/38336)    599/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.716% (12179/38400)  600/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.718% (12200/38464)    601/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.728% (12224/38528)    602/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.724% (12243/38592)    603/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.710% (12258/38656)    604/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.710% (12278/38720)    605/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.704% (12296/38784)    606/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.688% (12310/38848)  607/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.679% (12327/38912)    608/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.684% (12349/38976)    609/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 31.670% (12364/39040)    610/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 31.662% (12381/39104)    611/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 31.658% (12400/39168)    612/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 31.671% (12425/39232)    613/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 31.690% (12453/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.710% (12481/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.729 | Acc: 31.691% (12494/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.708% (12521/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.708% (12541/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.704% (12560/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.714% (12584/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.700% (12599/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.697% (12618/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.701% (12640/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.708% (12663/39936)  624/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.708% (12683/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.709% (12704/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.728 | Acc: 31.701% (12721/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.729 | Acc: 31.698% (12740/40192)    628/449 \n",
      " [=========================================>] | Loss: 1.729 | Acc: 31.696% (12751/40229)    629/449 \n",
      "=====train_acc====== tensor(31.6960) =====epoch===== 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 1.724 | Acc: 32.005% (9197/28736)     449/449 .....] | Loss: 1.794 | Acc: 33.594% (43/128)         2/449 ............] | Loss: 1.738 | Acc: 32.344% (207/640)        10/449 .........................] | Loss: 1.739 | Acc: 32.528% (229/704)        11/44 12/449 ...................] | Loss: 1.713 | Acc: 34.659% (488/1408)       22/449 ...........] | Loss: 1.715 | Acc: 34.237% (745/2176)       34/449 ..] | Loss: 1.720 | Acc: 33.767% (778/2304)       36/449 ...................] | Loss: 1.718 | Acc: 33.868% (802/2368)       37/449 .........................] | Loss: 1.721 | Acc: 33.799% (822/2432)       38/449  41/449 >..........................] | Loss: 1.730 | Acc: 32.779% (986/3008)       47/449 ..] | Loss: 1.728 | Acc: 32.719% (1047/3200)      50/449 ....] | Loss: 1.723 | Acc: 33.044% (1142/3456)      54/449 55/449 >..........................] | Loss: 1.727 | Acc: 33.147% (1188/3584)      56/449 .....................] | Loss: 1.725 | Acc: 33.432% (1241/3712)      58/449 ..............] | Loss: 1.724 | Acc: 33.385% (1282/3840)      60/449 ] | Loss: 1.724 | Acc: 33.376% (1303/3904)      61/449 >.........................] | Loss: 1.725 | Acc: 33.216% (1318/3968)      62/449 ........................] | Loss: 1.725 | Acc: 33.209% (1339/4032)      63/44 64/449 ====>.........................] | Loss: 1.726 | Acc: 33.149% (1379/4160)      65/449 68/449 73/44 74/449 ........................] | Loss: 1.729 | Acc: 32.667% (1568/4800)      75/449 ....] | Loss: 1.727 | Acc: 32.893% (1642/4992)      78/449 .................] | Loss: 1.727 | Acc: 32.832% (1660/5056)      79/449 82/44 89/449 .......] | Loss: 1.727 | Acc: 32.656% (1881/5760)      90/449 ===>.......................] | Loss: 1.726 | Acc: 32.675% (1903/5824)      91/449 92/449 ....................] | Loss: 1.726 | Acc: 32.476% (1933/5952)      93/449 ======>.......................] | Loss: 1.727 | Acc: 32.377% (2155/6656)      104/449 ..............] | Loss: 1.725 | Acc: 32.468% (2265/6976)      109/449 .....................] | Loss: 1.725 | Acc: 32.415% (2282/7040)      110/449 .....................] | Loss: 1.727 | Acc: 32.270% (2499/7744)      121/449 127/449 ========>.....................] | Loss: 1.731 | Acc: 32.176% (2780/8640)      135/449 =====>....................] | Loss: 1.731 | Acc: 32.151% (2819/8768)      137/449 ........] | Loss: 1.733 | Acc: 31.975% (2865/8960)      140/44 145/449 146/44 150/449 ==========>...................] | Loss: 1.731 | Acc: 32.109% (3103/9664)      151/449 ..........] | Loss: 1.733 | Acc: 32.151% (3354/10432)     163/449 ====>...................] | Loss: 1.733 | Acc: 32.174% (3377/10496)     164/449 ................] | Loss: 1.733 | Acc: 32.150% (3395/10560)     165/449 ========>..................] | Loss: 1.733 | Acc: 32.144% (3415/10624)     166/449 ..] | Loss: 1.733 | Acc: 32.158% (3437/10688)     167/44 184/449 =>.................] | Loss: 1.730 | Acc: 32.221% (3815/11840)     185/449 ...........] | Loss: 1.729 | Acc: 32.303% (3866/11968)     187/449 ==========>................] | Loss: 1.731 | Acc: 32.051% (4082/12736)     199/449     206/449 ...] | Loss: 1.728 | Acc: 32.126% (4256/13248)     207/449 ====>...............] | Loss: 1.730 | Acc: 31.918% (4351/13632)     213/449 216/449 ===============>..............] | Loss: 1.729 | Acc: 31.945% (4641/14528)     227/449 =====>..............] | Loss: 1.729 | Acc: 31.939% (4681/14656)     229/44 232/449 ==========>..............] | Loss: 1.729 | Acc: 31.927% (4761/14912)     233/449 ======>..............] | Loss: 1.729 | Acc: 31.903% (4839/15168)     237/449 ........] | Loss: 1.729 | Acc: 31.933% (4864/15232)     238/449 .............] | Loss: 1.729 | Acc: 31.910% (4881/15296)     239/44 254/449 ===========>.............] | Loss: 1.729 | Acc: 31.936% (5212/16320)     255/449 259/44 262/449 =======>............] | Loss: 1.730 | Acc: 31.921% (5373/16832)     263/449 ===>............] | Loss: 1.729 | Acc: 31.987% (5425/16960)     265/449 268/44 270/44 273/44 277/44 278/449 ...] | Loss: 1.729 | Acc: 31.930% (5824/18240)     285/449 ........] | Loss: 1.730 | Acc: 31.879% (5876/18432)     288/449 ===============>..........] | Loss: 1.730 | Acc: 31.803% (5984/18816)     294/449 ======>.........] | Loss: 1.730 | Acc: 31.797% (6166/19392)     303/449 ....] | Loss: 1.729 | Acc: 31.840% (6256/19648)     307/449 ====================>.........] | Loss: 1.730 | Acc: 31.864% (6383/20032)     313/44 333/449 349/449 =======================>......] | Loss: 1.725 | Acc: 31.953% (7178/22464)     351/449 ========>......] | Loss: 1.725 | Acc: 31.965% (7242/22656)     354/44 361/449 ===============>.....] | Loss: 1.726 | Acc: 31.900% (7513/23552)     368/449 .....] | Loss: 1.726 | Acc: 31.884% (7591/23808)     372/44 375/449 378/449 ....] | Loss: 1.725 | Acc: 31.983% (7983/24960)     390/449  391/449 ======================>...] | Loss: 1.726 | Acc: 31.952% (8016/25088)     392/449 =====================>...] | Loss: 1.725 | Acc: 31.974% (8165/25536)     399/449 ===================>..] | Loss: 1.725 | Acc: 31.981% (8310/25984)     406/44 407/449 ==================>..] | Loss: 1.725 | Acc: 31.926% (8459/26496)     414/449 =====================>..] | Loss: 1.725 | Acc: 31.943% (8484/26560)     415/44 418/44 428/449 =====================>.] | Loss: 1.724 | Acc: 31.949% (8772/27456)     429/449 =============>.] | Loss: 1.725 | Acc: 31.923% (8826/27648)     432/449 ==========================>] | Loss: 1.725 | Acc: 31.938% (8912/27904)     436/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 32.014% (9220/28800)     450/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 32.012% (9240/28864)   451/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 31.993% (9255/28928)     452/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 31.978% (9271/28992)     453/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 31.987% (9294/29056)     454/449 \n",
      " [==============================>] | Loss: 1.725 | Acc: 32.002% (9319/29120)     455/449 \n",
      " [==============================>] | Loss: 1.725 | Acc: 31.976% (9332/29184)     456/449 \n",
      " [==============================>] | Loss: 1.725 | Acc: 31.992% (9357/29248)     457/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 32.007% (9382/29312)     458/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 32.009% (9403/29376)     459/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 32.018% (9426/29440)     460/449 \n",
      " [==============================>] | Loss: 1.724 | Acc: 32.019% (9447/29504)   461/449 \n",
      " [==============================>] | Loss: 1.725 | Acc: 32.014% (9466/29568)     462/449 \n",
      " [==============================>] | Loss: 1.725 | Acc: 32.016% (9487/29632)     463/449 \n",
      " [==============================>] | Loss: 1.725 | Acc: 31.994% (9501/29696)     464/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.993% (9521/29760)     465/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.974% (9536/29824)     466/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.973% (9556/29888)     467/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.958% (9572/29952)     468/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.960% (9593/30016)     469/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.945% (9609/30080)     470/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.933% (9626/30144)     471/449 \n",
      " [===============================>] | Loss: 1.726 | Acc: 31.925% (9644/30208)     472/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.914% (9661/30272)   473/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.909% (9680/30336)     474/449 \n",
      " [===============================>] | Loss: 1.725 | Acc: 31.891% (9695/30400)     475/449 \n",
      " [===============================>] | Loss: 1.726 | Acc: 31.847% (9702/30464)     476/449 \n",
      " [===============================>] | Loss: 1.726 | Acc: 31.846% (9722/30528)     477/449 \n",
      " [===============================>] | Loss: 1.726 | Acc: 31.829% (9737/30592)     478/449 \n",
      " [===============================>] | Loss: 1.726 | Acc: 31.808% (9751/30656)     479/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================>] | Loss: 1.726 | Acc: 31.784% (9764/30720)     480/449 \n",
      " [================================>] | Loss: 1.726 | Acc: 31.767% (9779/30784)     481/449 \n",
      " [================================>] | Loss: 1.726 | Acc: 31.756% (9796/30848)     482/449 \n",
      " [================================>] | Loss: 1.727 | Acc: 31.726% (9807/30912)     483/449 \n",
      " [================================>] | Loss: 1.727 | Acc: 31.689% (9816/30976)     484/449 \n",
      " [================================>] | Loss: 1.727 | Acc: 31.672% (9831/31040)     485/449 \n",
      " [================================>] | Loss: 1.728 | Acc: 31.645% (9843/31104)     486/449 \n",
      " [================================>] | Loss: 1.728 | Acc: 31.641% (9862/31168)     487/449 \n",
      " [================================>] | Loss: 1.728 | Acc: 31.631% (9879/31232)     488/449 \n",
      " [================================>] | Loss: 1.728 | Acc: 31.621% (9896/31296)     489/449 \n",
      " [================================>] | Loss: 1.728 | Acc: 31.604% (9911/31360)     490/449 \n",
      " [================================>] | Loss: 1.728 | Acc: 31.603% (9931/31424)     491/449 \n",
      " [================================>] | Loss: 1.727 | Acc: 31.599% (9950/31488)   492/449 \n",
      " [================================>] | Loss: 1.727 | Acc: 31.605% (9972/31552)     493/449 \n",
      " [================================>] | Loss: 1.727 | Acc: 31.588% (9987/31616)   494/449 \n",
      " [=================================>] | Loss: 1.727 | Acc: 31.581% (10005/31680)    495/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.578% (10024/31744)    496/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.574% (10043/31808)    497/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.570% (10062/31872)    498/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.573% (10083/31936)    499/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.566% (10101/32000)    500/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.568% (10122/32064)    501/449 \n",
      " [=================================>] | Loss: 1.728 | Acc: 31.561% (10140/32128)    502/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.554% (10158/32192)    503/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.548% (10176/32256)    504/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.535% (10192/32320)    505/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.516% (10206/32384)    506/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.506% (10223/32448)    507/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.508% (10244/32512)    508/449 \n",
      " [=================================>] | Loss: 1.729 | Acc: 31.493% (10259/32576)  509/449 \n",
      " [==================================>] | Loss: 1.730 | Acc: 31.474% (10273/32640)    510/449 \n",
      " [==================================>] | Loss: 1.730 | Acc: 31.455% (10287/32704)    511/449 \n",
      " [==================================>] | Loss: 1.730 | Acc: 31.448% (10305/32768)    512/449 \n",
      " [==================================>] | Loss: 1.730 | Acc: 31.445% (10324/32832)    513/449 \n",
      " [==================================>] | Loss: 1.730 | Acc: 31.426% (10338/32896)    514/449 \n",
      " [==================================>] | Loss: 1.730 | Acc: 31.420% (10356/32960)    515/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.414% (10374/33024)    516/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.380% (10383/33088)    517/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.374% (10401/33152)    518/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.361% (10417/33216)    519/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.349% (10433/33280)    520/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.334% (10448/33344)    521/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.343% (10471/33408)    522/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.355% (10495/33472)    523/449 \n",
      " [==================================>] | Loss: 1.731 | Acc: 31.369% (10520/33536)  524/449 \n",
      " [===================================>] | Loss: 1.731 | Acc: 31.384% (10545/33600)    525/449 \n",
      " [===================================>] | Loss: 1.731 | Acc: 31.375% (10562/33664)    526/449 \n",
      " [===================================>] | Loss: 1.731 | Acc: 31.369% (10580/33728)    527/449 \n",
      " [===================================>] | Loss: 1.731 | Acc: 31.351% (10594/33792)    528/449 \n",
      " [===================================>] | Loss: 1.731 | Acc: 31.356% (10616/33856)  529/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.344% (10632/33920)    530/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.338% (10650/33984)    531/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.315% (10662/34048)    532/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.314% (10682/34112)    533/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.285% (10692/34176)    534/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.270% (10707/34240)    535/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.256% (10722/34304)    536/449 \n",
      " [===================================>] | Loss: 1.732 | Acc: 31.273% (10748/34368)    537/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 31.265% (10765/34432)    538/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 31.256% (10782/34496)    539/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.253% (10801/34560)    540/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.247% (10819/34624)    541/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.238% (10836/34688)    542/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.227% (10852/34752)    543/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.221% (10870/34816)  544/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.230% (10893/34880)    545/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.224% (10911/34944)    546/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 31.230% (10933/35008)    547/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 31.224% (10951/35072)    548/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 31.233% (10974/35136)    549/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 31.236% (10995/35200)  550/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 31.247% (11019/35264)    551/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.258% (11043/35328)    552/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.264% (11065/35392)    553/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 31.284% (11092/35456)    554/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.292% (11115/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.298% (11137/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.732 | Acc: 31.329% (11168/35648)    557/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.303% (11179/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.298% (11197/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.292% (11215/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.286% (11233/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.281% (11251/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.258% (11263/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.267% (11286/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.278% (11310/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.283% (11332/36224)    566/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=====================================>] | Loss: 1.733 | Acc: 31.280% (11351/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.291% (11375/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.733 | Acc: 31.299% (11398/36416)    569/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.299% (11418/36480)    570/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.307% (11441/36544)    571/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.305% (11460/36608)    572/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.302% (11479/36672)    573/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.291% (11495/36736)    574/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.302% (11519/36800)    575/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.293% (11536/36864)    576/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.301% (11559/36928)    577/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.318% (11585/36992)    578/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 31.296% (11597/37056)  579/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.304% (11620/37120)    580/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.298% (11638/37184)    581/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.301% (11659/37248)    582/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 31.290% (11675/37312)    583/449 \n",
      " [======================================>] | Loss: 1.733 | Acc: 31.295% (11697/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.282% (11712/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.282% (11732/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.293% (11756/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.295% (11777/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.284% (11793/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.300% (11819/37760)    590/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.324% (11848/37824)    591/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.342% (11875/37888)    592/449 \n",
      " [=======================================>] | Loss: 1.733 | Acc: 31.342% (11895/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.350% (11918/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.373% (11947/38080)  595/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.360% (11962/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.373% (11987/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.386% (12012/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.732 | Acc: 31.388% (12033/38336)    599/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.367% (12045/38400)    600/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.383% (12071/38464)    601/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.385% (12092/38528)    602/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.382% (12111/38592)    603/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.382% (12131/38656)    604/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.384% (12152/38720)    605/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.392% (12175/38784)    606/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.417% (12205/38848)    607/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.438% (12233/38912)    608/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.460% (12262/38976)    609/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.463% (12283/39040)    610/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.465% (12304/39104)    611/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.457% (12321/39168)    612/449 \n",
      " [========================================>] | Loss: 1.731 | Acc: 31.439% (12334/39232)    613/449 \n",
      " [========================================>] | Loss: 1.732 | Acc: 31.438% (12354/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.438% (12374/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.443% (12396/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.445% (12417/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.462% (12444/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.460% (12463/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.464% (12485/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.474% (12509/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.481% (12532/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.486% (12554/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.498% (12579/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.487% (12595/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.495% (12618/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.497% (12639/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.732 | Acc: 31.494% (12658/40192)    628/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 31.497% (12671/40229)    629/449 \n",
      "=====train_acc====== tensor(31.4972) =====epoch===== 3\n",
      " [=============================>] | Loss: 1.721 | Acc: 32.955% (9449/28672)     448/449 .......] | Loss: 1.678 | Acc: 36.049% (323/896)        14/449 .................] | Loss: 1.677 | Acc: 35.729% (343/960)        15/449 ..................] | Loss: 1.694 | Acc: 35.417% (476/1344)       21/449 22/449 ....................] | Loss: 1.694 | Acc: 34.625% (554/1600)       25/449       26/449 27/449 .................] | Loss: 1.690 | Acc: 35.100% (629/1792)       28/449 .....................] | Loss: 1.691 | Acc: 34.789% (757/2176)       34/449 ...........................] | Loss: 1.697 | Acc: 34.643% (776/2240)       35/449 36/449 ..............] | Loss: 1.701 | Acc: 34.048% (937/2752)       43/449 >...........................] | Loss: 1.702 | Acc: 34.236% (986/2880)       45/449 ........] | Loss: 1.700 | Acc: 34.408% (1035/3008)      47/449 .] | Loss: 1.698 | Acc: 34.312% (1098/3200)      50/449 ...] | Loss: 1.699 | Acc: 34.191% (1116/3264)      51/44 60/44 67/449     72/449 76/449 ......................] | Loss: 1.711 | Acc: 32.672% (1631/4992)      78/449 .........] | Loss: 1.714 | Acc: 32.474% (1725/5312)      83/449 ...................] | Loss: 1.713 | Acc: 32.515% (1748/5376)      84/449   85/449 ....] | Loss: 1.715 | Acc: 32.504% (1789/5504)      86/44 90/449 .......] | Loss: 1.713 | Acc: 32.510% (1935/5952)      93/449  96/449     100/449 101/449 ....................] | Loss: 1.712 | Acc: 32.721% (2136/6528)      102/449 >.......................] | Loss: 1.714 | Acc: 32.661% (2153/6592)      103/449 =======>......................] | Loss: 1.715 | Acc: 32.650% (2215/6784)      106/449  | Loss: 1.717 | Acc: 32.614% (2296/7040)      110/449 ====>......................] | Loss: 1.717 | Acc: 32.587% (2315/7104)      111/449  | Loss: 1.716 | Acc: 32.579% (2377/7296)      114/449  121/449 ...................] | Loss: 1.716 | Acc: 32.646% (2549/7808)      122/449 ...............] | Loss: 1.715 | Acc: 32.512% (2705/8320)      130/449 >.....................] | Loss: 1.716 | Acc: 32.455% (2721/8384)      131/449 ..................] | Loss: 1.716 | Acc: 32.401% (2758/8512)      133/449 .............] | Loss: 1.716 | Acc: 32.416% (2780/8576)      134/449 140/449 .............] | Loss: 1.714 | Acc: 32.548% (2958/9088)      142/449 147/449 152/449 ...............] | Loss: 1.716 | Acc: 32.670% (3199/9792)      153/449   156/449 .......] | Loss: 1.717 | Acc: 32.575% (3294/10112)     158/449 ...] | Loss: 1.717 | Acc: 32.586% (3316/10176)     159/449 ...........] | Loss: 1.718 | Acc: 32.631% (3425/10496)     164/449 ===========>..................] | Loss: 1.718 | Acc: 32.822% (3613/11008)     172/449 ===========>..................] | Loss: 1.718 | Acc: 32.849% (3637/11072)     173/449 .........] | Loss: 1.718 | Acc: 32.848% (3742/11392)     178/449 =>.................] | Loss: 1.719 | Acc: 32.942% (3816/11584)     181/44 189/449 190/449 =========>.................] | Loss: 1.721 | Acc: 32.983% (4074/12352)     193/449 ===>................] | Loss: 1.721 | Acc: 32.923% (4172/12672)     198/449  | Loss: 1.723 | Acc: 32.721% (4293/13120)     205/44 206/449 ...] | Loss: 1.722 | Acc: 32.643% (4429/13568)     212/449 ==========>...............] | Loss: 1.723 | Acc: 32.653% (4493/13760)     215/44 220/449 =========>...............] | Loss: 1.723 | Acc: 32.565% (4606/14144)     221/449 =>..............] | Loss: 1.722 | Acc: 32.656% (4786/14656)     229/449 235/449 ========>..............] | Loss: 1.722 | Acc: 32.656% (4995/15296)     239/449 247/449 248/449 .....] | Loss: 1.721 | Acc: 32.708% (5296/16192)     253/44 257/449 ] | Loss: 1.720 | Acc: 32.812% (5565/16960)     265/449 270/449 277/449 282/449 ==========>..........] | Loss: 1.722 | Acc: 32.797% (6150/18752)     293/449 ====================>.........] | Loss: 1.721 | Acc: 32.843% (6369/19392)     303/449 304/449 =====================>........] | Loss: 1.722 | Acc: 32.744% (6706/20480)     320/449 =====================>........] | Loss: 1.721 | Acc: 32.841% (6831/20800)     325/449 ================>.......] | Loss: 1.720 | Acc: 32.944% (7000/21248)     332/449 337/44 340/449 =========>......] | Loss: 1.718 | Acc: 33.001% (7350/22272)     348/449 363/44 364/449 ==============>.....] | Loss: 1.717 | Acc: 33.099% (7732/23360)     365/449 368/44 372/449 ========================>.....] | Loss: 1.717 | Acc: 33.076% (7917/23936)     374/449 =============>....] | Loss: 1.717 | Acc: 33.111% (7989/24128)     377/449 381/449 383/449 ===============>....] | Loss: 1.717 | Acc: 33.097% (8134/24576)     384/449 385/449     389/449 391/449 ============>...] | Loss: 1.717 | Acc: 33.119% (8309/25088)     392/449 394/44 395/449    402/449 =>...] | Loss: 1.716 | Acc: 33.152% (8593/25920)     405/449 ==>..] | Loss: 1.717 | Acc: 33.147% (8613/25984)     406/449 ====>..] | Loss: 1.717 | Acc: 33.114% (8774/26496)     414/449 ===============>.] | Loss: 1.719 | Acc: 32.984% (9035/27392)     428/449 438/449  | Loss: 1.720 | Acc: 32.919% (9249/28096)     439/449 ========================>] | Loss: 1.721 | Acc: 32.945% (9467/28736)     449/449 \n",
      " [==============================>] | Loss: 1.721 | Acc: 32.951% (9490/28800)     450/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.948% (9510/28864)     451/449 \n",
      " [==============================>] | Loss: 1.721 | Acc: 32.947% (9531/28928)     452/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.971% (9559/28992)     453/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.967% (9579/29056)     454/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.974% (9602/29120)     455/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.980% (9625/29184)     456/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.970% (9643/29248)     457/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.966% (9663/29312)     458/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.983% (9689/29376)     459/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.979% (9709/29440)     460/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.951% (9722/29504)     461/449 \n",
      " [==============================>] | Loss: 1.720 | Acc: 32.961% (9746/29568)     462/449 \n",
      " [==============================>] | Loss: 1.721 | Acc: 32.951% (9764/29632)     463/449 \n",
      " [==============================>] | Loss: 1.721 | Acc: 32.937% (9781/29696)     464/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.964% (9810/29760)     465/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.967% (9832/29824)     466/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.960% (9851/29888)     467/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.946% (9868/29952)     468/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.929% (9884/30016)     469/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.929% (9905/30080)     470/449 \n",
      " [===============================>] | Loss: 1.721 | Acc: 32.919% (9923/30144)     471/449 \n",
      " [===============================>] | Loss: 1.721 | Acc: 32.912% (9942/30208)   472/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.908% (9962/30272)     473/449 \n",
      " [===============================>] | Loss: 1.721 | Acc: 32.902% (9981/30336)     474/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.895% (10000/30400)    475/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.918% (10028/30464)    476/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.904% (10045/30528)    477/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.891% (10062/30592)    478/449 \n",
      " [===============================>] | Loss: 1.720 | Acc: 32.874% (10078/30656)    479/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.897% (10106/30720)    480/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.894% (10126/30784)    481/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.881% (10143/30848)    482/449 \n",
      " [================================>] | Loss: 1.721 | Acc: 32.858% (10157/30912)  483/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.871% (10182/30976)    484/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================>] | Loss: 1.720 | Acc: 32.870% (10203/31040)    485/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.890% (10230/31104)    486/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.893% (10252/31168)    487/449 \n",
      " [================================>] | Loss: 1.721 | Acc: 32.867% (10265/31232)    488/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.883% (10291/31296)    489/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.879% (10311/31360)    490/449 \n",
      " [================================>] | Loss: 1.720 | Acc: 32.886% (10334/31424)    491/449 \n",
      " [================================>] | Loss: 1.721 | Acc: 32.863% (10348/31488)    492/449 \n",
      " [================================>] | Loss: 1.721 | Acc: 32.860% (10368/31552)    493/449 \n",
      " [================================>] | Loss: 1.721 | Acc: 32.863% (10390/31616)    494/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.854% (10408/31680)    495/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.857% (10430/31744)    496/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.850% (10449/31808)    497/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.838% (10466/31872)  498/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.834% (10486/31936)    499/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.838% (10508/32000)    500/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.844% (10531/32064)  501/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.850% (10554/32128)    502/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.850% (10575/32192)    503/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.847% (10595/32256)    504/449 \n",
      " [=================================>] | Loss: 1.721 | Acc: 32.862% (10621/32320)    505/449 \n",
      " [=================================>] | Loss: 1.722 | Acc: 32.859% (10641/32384)    506/449 \n",
      " [=================================>] | Loss: 1.722 | Acc: 32.856% (10661/32448)    507/449 \n",
      " [=================================>] | Loss: 1.722 | Acc: 32.825% (10672/32512)    508/449 \n",
      " [=================================>] | Loss: 1.722 | Acc: 32.825% (10693/32576)    509/449 \n",
      " [==================================>] | Loss: 1.722 | Acc: 32.812% (10710/32640)    510/449 \n",
      " [==================================>] | Loss: 1.722 | Acc: 32.809% (10730/32704)  511/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.812% (10752/32768)    512/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.806% (10771/32832)    513/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.803% (10791/32896)    514/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.806% (10813/32960)    515/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.800% (10832/33024)    516/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.779% (10846/33088)    517/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.779% (10867/33152)    518/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.785% (10890/33216)    519/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.767% (10905/33280)    520/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.777% (10929/33344)    521/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.777% (10950/33408)    522/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.765% (10967/33472)    523/449 \n",
      " [==================================>] | Loss: 1.723 | Acc: 32.771% (10990/33536)    524/449 \n",
      " [===================================>] | Loss: 1.723 | Acc: 32.759% (11007/33600)  525/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.759% (11028/33664)    526/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.753% (11047/33728)    527/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.733% (11061/33792)    528/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.739% (11084/33856)    529/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.736% (11104/33920)    530/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.736% (11125/33984)  531/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.736% (11146/34048)    532/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.742% (11169/34112)    533/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.722% (11183/34176)    534/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.716% (11202/34240)    535/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.716% (11223/34304)    536/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.702% (11239/34368)    537/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.714% (11264/34432)    538/449 \n",
      " [===================================>] | Loss: 1.724 | Acc: 32.740% (11294/34496)    539/449 \n",
      " [====================================>] | Loss: 1.724 | Acc: 32.743% (11316/34560)    540/449 \n",
      " [====================================>] | Loss: 1.724 | Acc: 32.726% (11331/34624)    541/449 \n",
      " [====================================>] | Loss: 1.724 | Acc: 32.720% (11350/34688)    542/449 \n",
      " [====================================>] | Loss: 1.724 | Acc: 32.715% (11369/34752)    543/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.709% (11388/34816)    544/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.709% (11409/34880)    545/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.698% (11426/34944)    546/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.704% (11449/35008)    547/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.713% (11473/35072)    548/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.719% (11496/35136)    549/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.693% (11508/35200)    550/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.685% (11526/35264)    551/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.677% (11544/35328)    552/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.674% (11564/35392)    553/449 \n",
      " [====================================>] | Loss: 1.725 | Acc: 32.671% (11584/35456)    554/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.652% (11598/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.655% (11620/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.650% (11639/35648)    557/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.653% (11661/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.642% (11678/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.648% (11701/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.620% (11712/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.612% (11730/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.726 | Acc: 32.596% (11745/36032)  563/449 \n",
      " [=====================================>] | Loss: 1.727 | Acc: 32.594% (11765/36096)  564/449 \n",
      " [=====================================>] | Loss: 1.727 | Acc: 32.597% (11787/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.727 | Acc: 32.592% (11806/36224)    566/449 \n",
      " [=====================================>] | Loss: 1.727 | Acc: 32.600% (11830/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.727 | Acc: 32.595% (11849/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.727 | Acc: 32.585% (11866/36416)  569/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.560% (11878/36480)    570/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.563% (11900/36544)    571/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================>] | Loss: 1.727 | Acc: 32.556% (11918/36608)    572/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.545% (11935/36672)    573/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.540% (11954/36736)    574/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.549% (11978/36800)    575/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.560% (12003/36864)    576/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.542% (12017/36928)    577/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.531% (12034/36992)    578/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.526% (12053/37056)    579/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.513% (12069/37120)    580/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.501% (12085/37184)    581/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.509% (12109/37248)    582/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.523% (12135/37312)    583/449 \n",
      " [======================================>] | Loss: 1.727 | Acc: 32.526% (12157/37376)  584/449 \n",
      " [=======================================>] | Loss: 1.727 | Acc: 32.524% (12177/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.727 | Acc: 32.503% (12190/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.727 | Acc: 32.501% (12210/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.475% (12221/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.468% (12239/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.468% (12260/37760)    590/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.469% (12281/37824)    591/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.464% (12300/37888)    592/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.459% (12319/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.465% (12342/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.458% (12360/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.456% (12380/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.449% (12398/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.444% (12417/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.728 | Acc: 32.458% (12443/38336)    599/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 32.464% (12466/38400)    600/449 \n",
      " [========================================>] | Loss: 1.728 | Acc: 32.449% (12481/38464)    601/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.431% (12495/38528)    602/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.416% (12510/38592)    603/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.412% (12529/38656)    604/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.404% (12547/38720)    605/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.408% (12569/38784)  606/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.393% (12584/38848)    607/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.383% (12601/38912)    608/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.361% (12613/38976)    609/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.357% (12632/39040)    610/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.352% (12651/39104)    611/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.335% (12665/39168)    612/449 \n",
      " [========================================>] | Loss: 1.729 | Acc: 32.343% (12689/39232)    613/449 \n",
      " [========================================>] | Loss: 1.730 | Acc: 32.332% (12705/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.729 | Acc: 32.350% (12733/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.328% (12745/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.329% (12766/39488)  617/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.325% (12785/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.318% (12803/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.306% (12819/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.302% (12838/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.308% (12861/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.313% (12884/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.317% (12906/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.315% (12926/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.318% (12948/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.730 | Acc: 32.304% (12963/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 32.280% (12974/40192)    628/449 \n",
      " [=========================================>] | Loss: 1.731 | Acc: 32.278% (12985/40229)    629/449 \n",
      "=====train_acc====== tensor(32.2777) =====epoch===== 4\n",
      " [=============================>] | Loss: 1.759 | Acc: 29.077% (8337/28672)     448/449 ........................] | Loss: 1.736 | Acc: 32.031% (82/256)         4/449 ..........................] | Loss: 1.745 | Acc: 31.771% (122/384)        6/449        7/449        10/449 11/449 14/449 =>............................] | Loss: 1.739 | Acc: 32.617% (334/1024)       16/449 ....................] | Loss: 1.750 | Acc: 31.194% (559/1792)       28/449 .........................] | Loss: 1.751 | Acc: 31.351% (622/1984)       31/449 .....................] | Loss: 1.752 | Acc: 31.487% (665/2112)       33/449 ...........] | Loss: 1.753 | Acc: 31.526% (686/2176)       34/449 37/449 .............] | Loss: 1.756 | Acc: 31.143% (877/2816)       44/449 ............] | Loss: 1.764 | Acc: 30.199% (1063/3520)      55/449 59/449 .......] | Loss: 1.765 | Acc: 29.712% (1198/4032)      63/449 ====>.........................] | Loss: 1.765 | Acc: 29.397% (1317/4480)      70/449 79/449 83/449 94/449  98/449  108/44 123/449 .............] | Loss: 1.767 | Acc: 29.133% (2312/7936)      124/449 >.....................] | Loss: 1.766 | Acc: 29.175% (2334/8000)      125/449 =====>.....................] | Loss: 1.765 | Acc: 29.167% (2352/8064)      126/449 .....................] | Loss: 1.765 | Acc: 29.195% (2373/8128)      127/449 ....] | Loss: 1.763 | Acc: 29.288% (2493/8512)      133/449 .] | Loss: 1.763 | Acc: 29.204% (2598/8896)      139/449 ....................] | Loss: 1.764 | Acc: 29.208% (2617/8960)      140/449 ..................] | Loss: 1.764 | Acc: 29.156% (2631/9024)      141/449 ..............] | Loss: 1.764 | Acc: 29.108% (2664/9152)      143/449 ==>...................] | Loss: 1.762 | Acc: 29.156% (2855/9792)      153/449 >...................] | Loss: 1.762 | Acc: 29.133% (2890/9920)      155/449 ..........] | Loss: 1.761 | Acc: 29.186% (2970/10176)     159/449 .] | Loss: 1.761 | Acc: 29.245% (3107/10624)     166/449 >..................] | Loss: 1.762 | Acc: 29.167% (3136/10752)     168/449 ==>..................] | Loss: 1.762 | Acc: 29.151% (3153/10816)     169/449 ..................] | Loss: 1.761 | Acc: 29.285% (3205/10944)     171/449     172/449    173/44 174/449 177/449 ............] | Loss: 1.761 | Acc: 29.267% (3409/11648)     182/449  | Loss: 1.761 | Acc: 29.231% (3461/11840)     185/449 ........] | Loss: 1.758 | Acc: 29.446% (3656/12416)     194/449 201/44 217/449 .............] | Loss: 1.761 | Acc: 29.395% (4120/14016)     219/449 ==============>...............] | Loss: 1.761 | Acc: 29.428% (4200/14272)     223/449 =====>...............] | Loss: 1.761 | Acc: 29.429% (4219/14336)     224/449 ===========>..............] | Loss: 1.761 | Acc: 29.457% (4336/14720)     230/449 238/449 ==========>.............] | Loss: 1.761 | Acc: 29.415% (4537/15424)     241/44 242/449 ======>.............] | Loss: 1.762 | Acc: 29.383% (4626/15744)     246/449 ================>.............] | Loss: 1.762 | Acc: 29.333% (4712/16064)     251/449 ...] | Loss: 1.761 | Acc: 29.345% (4883/16640)     260/449 278/449  281/449 ========>..........] | Loss: 1.761 | Acc: 29.145% (5428/18624)     291/449 ...] | Loss: 1.762 | Acc: 29.075% (5508/18944)     296/449 >..........] | Loss: 1.762 | Acc: 29.056% (5523/19008)     297/449 320/44 329/449 =============>.......] | Loss: 1.761 | Acc: 28.991% (6160/21248)     332/449 ====================>......] | Loss: 1.761 | Acc: 28.978% (6491/22400)     350/449 352/449 354/449 ==========>......] | Loss: 1.761 | Acc: 28.979% (6584/22720)     355/449 357/449 ====================>.....] | Loss: 1.761 | Acc: 29.031% (6763/23296)     364/449 ===========>.....] | Loss: 1.761 | Acc: 29.045% (6785/23360)     365/449 ================>.....] | Loss: 1.761 | Acc: 29.068% (6809/23424)     366/449 =================>.....] | Loss: 1.762 | Acc: 29.066% (6827/23488)     367/449  368/44 374/44 382/44 396/449 ==================>...] | Loss: 1.761 | Acc: 29.008% (7389/25472)     398/449 ============>...] | Loss: 1.761 | Acc: 29.026% (7412/25536)     399/449 =========>...] | Loss: 1.761 | Acc: 28.988% (7421/25600)     400/44 406/449 ===========================>..] | Loss: 1.761 | Acc: 28.947% (7540/26048)     407/449 ===========================>..] | Loss: 1.761 | Acc: 28.956% (7561/26112)     408/449 419/449 =======================>..] | Loss: 1.761 | Acc: 29.014% (7799/26880)     420/449 ============================>.] | Loss: 1.760 | Acc: 29.007% (7890/27200)     425/449 ==============>.] | Loss: 1.760 | Acc: 29.043% (7937/27328)     427/449 ====>.] | Loss: 1.760 | Acc: 29.049% (7957/27392)     428/449 ==================>.] | Loss: 1.760 | Acc: 29.056% (8052/27712)     433/449 434/449 =======================>] | Loss: 1.760 | Acc: 29.049% (8143/28032)     438/449  | Loss: 1.760 | Acc: 29.072% (8168/28096)     439/449 ==============>] | Loss: 1.759 | Acc: 29.079% (8356/28736)     449/449 \n",
      " [==============================>] | Loss: 1.759 | Acc: 29.083% (8376/28800)     450/449 \n",
      " [==============================>] | Loss: 1.759 | Acc: 29.088% (8396/28864)   451/449 \n",
      " [==============================>] | Loss: 1.759 | Acc: 29.110% (8421/28928)     452/449 \n",
      " [==============================>] | Loss: 1.759 | Acc: 29.118% (8442/28992)     453/449 \n",
      " [==============================>] | Loss: 1.759 | Acc: 29.127% (8463/29056)     454/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.155% (8490/29120)     455/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.156% (8509/29184)   456/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.161% (8529/29248)     457/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.179% (8553/29312)     458/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.187% (8574/29376)     459/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.171% (8588/29440)     460/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.166% (8605/29504)     461/449 \n",
      " [==============================>] | Loss: 1.758 | Acc: 29.180% (8628/29568)     462/449 \n",
      " [==============================>] | Loss: 1.757 | Acc: 29.191% (8650/29632)     463/449 \n",
      " [==============================>] | Loss: 1.757 | Acc: 29.216% (8676/29696)   464/449 \n",
      " [===============================>] | Loss: 1.757 | Acc: 29.200% (8690/29760)     465/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.185% (8704/29824)     466/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.162% (8716/29888)     467/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.180% (8740/29952)     468/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.158% (8752/30016)   469/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.136% (8764/30080)     470/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.127% (8780/30144)     471/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.108% (8793/30208)     472/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.109% (8812/30272)     473/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.101% (8828/30336)     474/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.115% (8851/30400)     475/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.129% (8874/30464)     476/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.108% (8886/30528)     477/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.112% (8906/30592)     478/449 \n",
      " [===============================>] | Loss: 1.758 | Acc: 29.110% (8924/30656)     479/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.118% (8945/30720)     480/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.129% (8967/30784)     481/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.123% (8984/30848)     482/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.121% (9002/30912)     483/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.126% (9022/30976)     484/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.111% (9036/31040)     485/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.115% (9056/31104)     486/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================>] | Loss: 1.758 | Acc: 29.132% (9080/31168)     487/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.137% (9100/31232)     488/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.109% (9110/31296)   489/449 \n",
      " [================================>] | Loss: 1.759 | Acc: 29.120% (9132/31360)     490/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.137% (9156/31424)     491/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.132% (9173/31488)     492/449 \n",
      " [================================>] | Loss: 1.758 | Acc: 29.127% (9190/31552)     493/449 \n",
      " [================================>] | Loss: 1.759 | Acc: 29.128% (9209/31616)   494/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.100% (9219/31680)     495/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.086% (9233/31744)     496/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.084% (9251/31808)     497/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.079% (9268/31872)     498/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.080% (9287/31936)     499/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.081% (9306/32000)     500/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.092% (9328/32064)     501/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.112% (9353/32128)     502/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.110% (9371/32192)     503/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.108% (9389/32256)     504/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.103% (9406/32320)     505/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.095% (9422/32384)     506/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.096% (9441/32448)     507/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.088% (9457/32512)     508/449 \n",
      " [=================================>] | Loss: 1.759 | Acc: 29.074% (9471/32576)   509/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.066% (9487/32640)   510/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.067% (9506/32704)     511/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.074% (9527/32768)   512/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.078% (9547/32832)     513/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.095% (9571/32896)     514/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.105% (9593/32960)     515/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.106% (9612/33024)     516/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.116% (9634/33088)   517/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.114% (9652/33152)     518/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.118% (9672/33216)     519/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.099% (9684/33280)     520/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.097% (9702/33344)   521/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.080% (9715/33408)     522/449 \n",
      " [==================================>] | Loss: 1.759 | Acc: 29.078% (9733/33472)     523/449 \n",
      " [==================================>] | Loss: 1.758 | Acc: 29.094% (9757/33536)     524/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.113% (9782/33600)     525/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.114% (9801/33664)     526/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.130% (9825/33728)     527/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.128% (9843/33792)     528/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.132% (9863/33856)     529/449 \n",
      " [===================================>] | Loss: 1.757 | Acc: 29.154% (9889/33920)     530/449 \n",
      " [===================================>] | Loss: 1.757 | Acc: 29.155% (9908/33984)     531/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.153% (9926/34048)     532/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.166% (9949/34112)     533/449 \n",
      " [===================================>] | Loss: 1.757 | Acc: 29.175% (9971/34176)     534/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.165% (9986/34240)     535/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.172% (10007/34304)    536/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.178% (10028/34368)    537/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.171% (10044/34432)    538/449 \n",
      " [===================================>] | Loss: 1.758 | Acc: 29.174% (10064/34496)    539/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.155% (10076/34560)    540/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.150% (10093/34624)    541/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.146% (10110/34688)    542/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.149% (10130/34752)  543/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.136% (10144/34816)  544/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.149% (10167/34880)    545/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.149% (10186/34944)    546/449 \n",
      " [====================================>] | Loss: 1.759 | Acc: 29.136% (10200/35008)    547/449 \n",
      " [====================================>] | Loss: 1.759 | Acc: 29.146% (10222/35072)    548/449 \n",
      " [====================================>] | Loss: 1.759 | Acc: 29.138% (10238/35136)    549/449 \n",
      " [====================================>] | Loss: 1.759 | Acc: 29.165% (10266/35200)    550/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.191% (10294/35264)    551/449 \n",
      " [====================================>] | Loss: 1.759 | Acc: 29.184% (10310/35328)    552/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.207% (10337/35392)    553/449 \n",
      " [====================================>] | Loss: 1.758 | Acc: 29.216% (10359/35456)    554/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.229% (10382/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.227% (10400/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.230% (10420/35648)    557/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.225% (10437/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.207% (10449/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.213% (10470/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.214% (10489/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.198% (10502/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.760 | Acc: 29.177% (10513/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.183% (10534/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.176% (10550/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.177% (10569/36224)    566/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.186% (10591/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.190% (10611/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.759 | Acc: 29.179% (10626/36416)    569/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.164% (10639/36480)    570/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.159% (10656/36544)    571/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.166% (10677/36608)    572/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.153% (10691/36672)  573/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.146% (10707/36736)    574/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.149% (10727/36800)    575/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.145% (10744/36864)    576/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.146% (10763/36928)    577/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.171% (10791/36992)    578/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.167% (10808/37056)    579/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.159% (10824/37120)    580/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.158% (10842/37184)    581/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.161% (10862/37248)    582/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.160% (10880/37312)    583/449 \n",
      " [======================================>] | Loss: 1.759 | Acc: 29.155% (10897/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.140% (10910/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.144% (10930/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.123% (10941/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.111% (10955/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.114% (10975/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.105% (10990/37760)  590/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.109% (11010/37824)    591/449 \n",
      " [=======================================>] | Loss: 1.758 | Acc: 29.123% (11034/37888)  592/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.108% (11047/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.104% (11064/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.112% (11086/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.119% (11107/38144)  596/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.112% (11123/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.115% (11143/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.759 | Acc: 29.114% (11161/38336)    599/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.117% (11181/38400)  600/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.108% (11196/38464)    601/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.119% (11219/38528)  602/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.120% (11238/38592)  603/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.126% (11259/38656)    604/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.127% (11278/38720)    605/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.149% (11305/38784)    606/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.147% (11323/38848)    607/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.153% (11344/38912)    608/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.167% (11368/38976)    609/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.160% (11384/39040)    610/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.166% (11405/39104)  611/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.169% (11425/39168)    612/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.185% (11450/39232)    613/449 \n",
      " [========================================>] | Loss: 1.759 | Acc: 29.184% (11468/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.184% (11487/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.180% (11504/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.186% (11525/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.177% (11540/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.183% (11561/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.186% (11581/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.172% (11594/39744)  621/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.188% (11619/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.193% (11640/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.174% (11651/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.157% (11663/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.161% (11683/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.167% (11704/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.187% (11731/40192)  628/449 \n",
      " [=========================================>] | Loss: 1.759 | Acc: 29.183% (11740/40229)    629/449 \n",
      "=====train_acc====== tensor(29.1829) =====epoch===== 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 1.734 | Acc: 30.787% (8847/28736)     448/449 /449 >.............................] | Loss: 1.734 | Acc: 29.427% (113/384)        6/449 15/449 =>............................] | Loss: 1.746 | Acc: 29.167% (336/1152)       18/449 ...........................] | Loss: 1.742 | Acc: 29.792% (572/1920)       30/449 31/449 34/449 ..............] | Loss: 1.745 | Acc: 29.464% (660/2240)       35/449 .......................] | Loss: 1.744 | Acc: 29.603% (701/2368)       37/449 .......................] | Loss: 1.744 | Acc: 29.527% (737/2496)       39/449 .................] | Loss: 1.741 | Acc: 29.106% (801/2752)       43/449 45/449 50/449 .....................] | Loss: 1.737 | Acc: 29.369% (1015/3456)      54/449 ....................] | Loss: 1.738 | Acc: 29.325% (1051/3584)      56/449 ===>..........................] | Loss: 1.738 | Acc: 29.386% (1072/3648)      57/449 >..........................] | Loss: 1.738 | Acc: 29.555% (1116/3776)      59/44 60/44 66/449 .....................] | Loss: 1.740 | Acc: 29.340% (1352/4608)      72/449 .....................] | Loss: 1.740 | Acc: 29.366% (1372/4672)      73/449 81/449      82/449 100/449  101/449 ===>.......................] | Loss: 1.736 | Acc: 30.147% (1968/6528)      102/44 110/449 ====>......................] | Loss: 1.731 | Acc: 30.674% (2238/7296)      114/449 ..................] | Loss: 1.730 | Acc: 30.961% (2358/7616)      119/449 .....................] | Loss: 1.730 | Acc: 30.977% (2498/8064)      126/449 ......] | Loss: 1.730 | Acc: 30.947% (2555/8256)      129/449  | Loss: 1.732 | Acc: 30.815% (2623/8512)      133/449 134/449 .................] | Loss: 1.733 | Acc: 30.871% (2687/8704)      136/449 =====>....................] | Loss: 1.734 | Acc: 30.896% (2709/8768)      137/449 =====>...................] | Loss: 1.732 | Acc: 30.946% (3050/9856)      154/449  161/449 ..] | Loss: 1.733 | Acc: 30.932% (3207/10368)     162/449 174/449 =>..................] | Loss: 1.731 | Acc: 31.295% (3505/11200)     175/449 ===========>..................] | Loss: 1.731 | Acc: 31.277% (3523/11264)     176/449 >..................] | Loss: 1.731 | Acc: 31.285% (3544/11328)     177/44 201/449 .....] | Loss: 1.733 | Acc: 31.120% (4063/13056)     204/449 206/449 ======>...............] | Loss: 1.734 | Acc: 31.139% (4205/13504)     211/449 =====>...............] | Loss: 1.735 | Acc: 31.083% (4277/13760)     215/449 ===>...............] | Loss: 1.736 | Acc: 31.056% (4333/13952)     218/449 =====>..............] | Loss: 1.736 | Acc: 31.113% (4540/14592)     228/44 229/449 247/449 251/449 ====>.............] | Loss: 1.736 | Acc: 30.992% (5038/16256)     254/449 =================>............] | Loss: 1.736 | Acc: 31.021% (5142/16576)     259/449 ==============>............] | Loss: 1.736 | Acc: 31.049% (5246/16896)     264/449 278/449 ...........] | Loss: 1.736 | Acc: 31.055% (5585/17984)     281/449 =======>..........] | Loss: 1.737 | Acc: 31.041% (5801/18688)     292/449 =========>..........] | Loss: 1.736 | Acc: 31.059% (5864/18880)     295/449 ========>..........] | Loss: 1.736 | Acc: 31.050% (5902/19008)     297/449 ........] | Loss: 1.735 | Acc: 31.078% (5967/19200)     300/449 301/449 302/449    303/449 ..] | Loss: 1.734 | Acc: 31.019% (6174/19904)     311/44 323/449 326/449 ===============>........] | Loss: 1.737 | Acc: 30.838% (6513/21120)     330/449 354/449 ====================>.....] | Loss: 1.736 | Acc: 30.822% (7200/23360)     365/449 =============>.....] | Loss: 1.736 | Acc: 30.756% (7342/23872)     373/449 =========================>....] | Loss: 1.735 | Acc: 30.787% (7586/24640)     385/449 ================>....] | Loss: 1.735 | Acc: 30.802% (7629/24768)     387/449 390/449 411/449 ============>..] | Loss: 1.735 | Acc: 30.663% (8144/26560)     415/449 416/44 417/449 ============================>.] | Loss: 1.734 | Acc: 30.711% (8314/27072)     423/449     424/449 =====================>.] | Loss: 1.734 | Acc: 30.741% (8401/27328)     427/449 =>.] | Loss: 1.734 | Acc: 30.753% (8483/27584)     431/449 ==================>.] | Loss: 1.734 | Acc: 30.748% (8521/27712)     433/449 ========>.] | Loss: 1.734 | Acc: 30.739% (8538/27776)     434/449 ======>] | Loss: 1.734 | Acc: 30.756% (8661/28160)     440/44 441/44 444/449  449/449 \n",
      " [==============================>] | Loss: 1.734 | Acc: 30.799% (8870/28800)   450/449 \n",
      " [==============================>] | Loss: 1.734 | Acc: 30.789% (8887/28864)     451/449 \n",
      " [==============================>] | Loss: 1.734 | Acc: 30.818% (8915/28928)     452/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.836% (8940/28992)     453/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.837% (8960/29056)     454/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.852% (8984/29120)     455/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.859% (9006/29184)     456/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.853% (9024/29248)     457/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.830% (9037/29312)     458/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.842% (9060/29376)     459/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.842% (9080/29440)     460/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.840% (9099/29504)     461/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.864% (9126/29568)     462/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.845% (9140/29632)     463/449 \n",
      " [==============================>] | Loss: 1.733 | Acc: 30.839% (9158/29696)     464/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.823% (9173/29760)     465/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.828% (9194/29824)     466/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.822% (9212/29888)     467/449 \n",
      " [===============================>] | Loss: 1.734 | Acc: 30.833% (9235/29952)     468/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.834% (9255/30016)     469/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.841% (9277/30080)     470/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.829% (9293/30144)     471/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.826% (9312/30208)     472/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.837% (9335/30272)     473/449 \n",
      " [===============================>] | Loss: 1.734 | Acc: 30.818% (9349/30336)     474/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.816% (9368/30400)     475/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.833% (9393/30464)     476/449 \n",
      " [===============================>] | Loss: 1.733 | Acc: 30.831% (9412/30528)     477/449 \n",
      " [===============================>] | Loss: 1.734 | Acc: 30.828% (9431/30592)     478/449 \n",
      " [===============================>] | Loss: 1.734 | Acc: 30.813% (9446/30656)     479/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.827% (9470/30720)     480/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.831% (9491/30784)     481/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.848% (9516/30848)     482/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.849% (9536/30912)     483/449 \n",
      " [================================>] | Loss: 1.734 | Acc: 30.840% (9553/30976)     484/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.863% (9580/31040)     485/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.851% (9596/31104)     486/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.859% (9618/31168)     487/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.872% (9642/31232)     488/449 \n",
      " [================================>] | Loss: 1.733 | Acc: 30.879% (9664/31296)     489/449 \n",
      " [================================>] | Loss: 1.732 | Acc: 30.909% (9693/31360)     490/449 \n",
      " [================================>] | Loss: 1.732 | Acc: 30.919% (9716/31424)     491/449 \n",
      " [================================>] | Loss: 1.732 | Acc: 30.923% (9737/31488)     492/449 \n",
      " [================================>] | Loss: 1.732 | Acc: 30.933% (9760/31552)     493/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================>] | Loss: 1.732 | Acc: 30.946% (9784/31616)     494/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.947% (9804/31680)     495/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.957% (9827/31744)     496/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.964% (9849/31808)   497/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.964% (9869/31872)     498/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.959% (9887/31936)     499/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.972% (9911/32000)     500/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.963% (9928/32064)     501/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.976% (9952/32128)     502/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.989% (9976/32192)     503/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.990% (9996/32256)     504/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.981% (10013/32320)    505/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.991% (10036/32384)    506/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.985% (10054/32448)    507/449 \n",
      " [=================================>] | Loss: 1.732 | Acc: 30.976% (10071/32512)    508/449 \n",
      " [=================================>] | Loss: 1.733 | Acc: 30.958% (10085/32576)    509/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.947% (10101/32640)    510/449 \n",
      " [==================================>] | Loss: 1.732 | Acc: 30.966% (10127/32704)    511/449 \n",
      " [==================================>] | Loss: 1.732 | Acc: 30.972% (10149/32768)    512/449 \n",
      " [==================================>] | Loss: 1.732 | Acc: 30.952% (10162/32832)    513/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.955% (10183/32896)    514/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.944% (10199/32960)    515/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.944% (10219/33024)    516/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.930% (10234/33088)    517/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.918% (10250/33152)    518/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.907% (10266/33216)    519/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.919% (10290/33280)    520/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.896% (10302/33344)    521/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.879% (10316/33408)    522/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.897% (10342/33472)    523/449 \n",
      " [==================================>] | Loss: 1.733 | Acc: 30.895% (10361/33536)    524/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.893% (10380/33600)    525/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.882% (10396/33664)    526/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.873% (10413/33728)    527/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.874% (10433/33792)    528/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.863% (10449/33856)    529/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.849% (10464/33920)    530/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.829% (10477/33984)    531/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.809% (10490/34048)    532/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.816% (10512/34112)    533/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.817% (10532/34176)    534/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.818% (10552/34240)    535/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.833% (10577/34304)    536/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.834% (10597/34368)    537/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.835% (10617/34432)    538/449 \n",
      " [===================================>] | Loss: 1.733 | Acc: 30.859% (10645/34496)    539/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 30.851% (10662/34560)    540/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 30.854% (10683/34624)    541/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 30.855% (10703/34688)    542/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 30.867% (10727/34752)    543/449 \n",
      " [====================================>] | Loss: 1.733 | Acc: 30.848% (10740/34816)    544/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.843% (10758/34880)    545/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.832% (10774/34944)    546/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.839% (10796/35008)    547/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.839% (10816/35072)    548/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.843% (10837/35136)    549/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.832% (10853/35200)    550/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.839% (10875/35264)    551/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.848% (10898/35328)    552/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.829% (10911/35392)    553/449 \n",
      " [====================================>] | Loss: 1.734 | Acc: 30.838% (10934/35456)  554/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.847% (10957/35520)    555/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.831% (10971/35584)    556/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.818% (10986/35648)  557/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.838% (11013/35712)    558/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.842% (11034/35776)    559/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.840% (11053/35840)    560/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.835% (11071/35904)    561/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.847% (11095/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.845% (11114/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.832% (11129/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.832% (11149/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.825% (11166/36224)    566/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.823% (11185/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.843% (11212/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.734 | Acc: 30.838% (11230/36416)    569/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.844% (11252/36480)    570/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.861% (11278/36544)    571/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.854% (11295/36608)    572/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.838% (11309/36672)    573/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.831% (11326/36736)    574/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.823% (11343/36800)    575/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.832% (11366/36864)    576/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.828% (11384/36928)    577/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.831% (11405/36992)    578/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.829% (11424/37056)    579/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.838% (11447/37120)    580/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================>] | Loss: 1.734 | Acc: 30.833% (11465/37184)    581/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.834% (11485/37248)    582/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.821% (11500/37312)    583/449 \n",
      " [======================================>] | Loss: 1.734 | Acc: 30.819% (11519/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.817% (11538/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.815% (11557/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.811% (11575/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.804% (11592/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.818% (11617/37696)  589/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.842% (11646/37760)    590/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.845% (11667/37824)    591/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.846% (11687/37888)  592/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.842% (11705/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.855% (11730/38016)  594/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.864% (11753/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.734 | Acc: 30.849% (11767/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.735 | Acc: 30.850% (11787/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.735 | Acc: 30.835% (11801/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.735 | Acc: 30.843% (11824/38336)    599/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.836% (11841/38400)    600/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.834% (11860/38464)    601/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.840% (11882/38528)    602/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.830% (11898/38592)    603/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.818% (11913/38656)    604/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.824% (11935/38720)    605/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.840% (11961/38784)    606/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.830% (11977/38848)    607/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.821% (11993/38912)    608/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.822% (12013/38976)    609/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.820% (12032/39040)    610/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.831% (12056/39104)    611/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.831% (12076/39168)    612/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.824% (12093/39232)    613/449 \n",
      " [========================================>] | Loss: 1.735 | Acc: 30.828% (12114/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.735 | Acc: 30.831% (12135/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.735 | Acc: 30.814% (12148/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.804% (12164/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.805% (12184/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.816% (12208/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.804% (12223/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.795% (12239/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.780% (12253/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.794% (12278/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.777% (12291/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.778% (12311/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.778% (12331/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.736 | Acc: 30.769% (12347/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.737 | Acc: 30.757% (12362/40192)    628/449 \n",
      " [=========================================>] | Loss: 1.737 | Acc: 30.756% (12373/40229)    629/449 \n",
      "=====train_acc====== tensor(30.7564) =====epoch===== 6\n",
      " [=============================>] | Loss: 1.782 | Acc: 27.847% (8002/28736)     449/449 .......................] | Loss: 1.786 | Acc: 30.000% (96/320)         5/449 11/449 .............] | Loss: 1.800 | Acc: 28.562% (457/1600)       25/449 27/44 28/449 .........................] | Loss: 1.796 | Acc: 28.038% (646/2304)       36/449 45/449 50/449 ..................] | Loss: 1.797 | Acc: 27.114% (885/3264)       51/449 ........................] | Loss: 1.796 | Acc: 27.064% (918/3392)       53/449 56/449 .....................] | Loss: 1.797 | Acc: 26.915% (1068/3968)      62/449 63/449 .................] | Loss: 1.792 | Acc: 27.511% (1567/5696)      89/449 ......................] | Loss: 1.792 | Acc: 27.455% (1599/5824)      91/44 92/449 94/449 ..................] | Loss: 1.793 | Acc: 27.533% (1674/6080)      95/449 96/449 .....] | Loss: 1.793 | Acc: 27.471% (1723/6272)      98/449 103/44 104/449 .......] | Loss: 1.790 | Acc: 27.653% (1876/6784)      106/449 123/449 ...................] | Loss: 1.791 | Acc: 27.464% (2285/8320)      130/449 .................] | Loss: 1.791 | Acc: 27.445% (2301/8384)      131/449 ====>.....................] | Loss: 1.791 | Acc: 27.498% (2323/8448)      132/449 ========>.....................] | Loss: 1.791 | Acc: 27.414% (2351/8576)      134/449 ======>....................] | Loss: 1.792 | Acc: 27.355% (2416/8832)      138/449 .................] | Loss: 1.792 | Acc: 27.413% (2579/9408)      147/449 149/449 ====>....................] | Loss: 1.793 | Acc: 27.448% (2635/9600)      150/44 158/449 =======>..................] | Loss: 1.791 | Acc: 27.589% (2984/10816)     169/449 ........] | Loss: 1.791 | Acc: 27.610% (3057/11072)     173/449 ......] | Loss: 1.790 | Acc: 27.654% (3115/11264)     176/449 .....] | Loss: 1.790 | Acc: 27.666% (3134/11328)     177/44 179/449 ........] | Loss: 1.790 | Acc: 27.675% (3259/11776)     184/449 207/449 ==>...............] | Loss: 1.789 | Acc: 27.236% (3800/13952)     218/449 220/449 .............] | Loss: 1.789 | Acc: 27.256% (3890/14272)     223/449 ..........] | Loss: 1.789 | Acc: 27.237% (3957/14528)     227/449 ............] | Loss: 1.789 | Acc: 27.228% (4008/14720)     230/44 233/449 ====>..............] | Loss: 1.789 | Acc: 27.191% (4107/15104)     236/449 =========>..............] | Loss: 1.789 | Acc: 27.147% (4135/15232)     238/44 239/449 242/449 =======>.............] | Loss: 1.790 | Acc: 27.085% (4299/15872)     248/449  | Loss: 1.790 | Acc: 27.112% (4338/16000)     250/449 =>............] | Loss: 1.789 | Acc: 27.250% (4482/16448)     257/449 ..........] | Loss: 1.788 | Acc: 27.308% (4579/16768)     262/449 =============>............] | Loss: 1.788 | Acc: 27.329% (4600/16832)     263/449 270/449 ..] | Loss: 1.788 | Acc: 27.361% (4833/17664)     276/449 ==================>...........] | Loss: 1.788 | Acc: 27.341% (4882/17856)     279/44 287/449 ================>..........] | Loss: 1.788 | Acc: 27.325% (5054/18496)     289/44 292/449 ==>..........] | Loss: 1.788 | Acc: 27.307% (5138/18816)     294/449 300/449 =======>.........] | Loss: 1.789 | Acc: 27.311% (5366/19648)     307/449 ==>.........] | Loss: 1.789 | Acc: 27.341% (5407/19776)     309/449 ...] | Loss: 1.788 | Acc: 27.354% (5427/19840)     310/44 319/449 321/449 ==>........] | Loss: 1.788 | Acc: 27.373% (5676/20736)     324/44 326/449 ===================>........] | Loss: 1.788 | Acc: 27.408% (5736/20928)     327/44 330/449 =========>.......] | Loss: 1.786 | Acc: 27.531% (6026/21888)     342/449 >......] | Loss: 1.786 | Acc: 27.613% (6256/22656)     354/449 369/449     373/449 =======>....] | Loss: 1.784 | Acc: 27.830% (6697/24064)     376/44 379/44 382/449 =================>....] | Loss: 1.783 | Acc: 27.880% (6834/24512)     383/449 ======>....] | Loss: 1.783 | Acc: 27.878% (6887/24704)     386/449 ======================>....] | Loss: 1.783 | Acc: 27.843% (6914/24832)     388/44 391/449 ===========>...] | Loss: 1.784 | Acc: 27.799% (6992/25152)     393/449 ===>...] | Loss: 1.783 | Acc: 27.833% (7054/25344)     396/449 402/449 ======>..] | Loss: 1.783 | Acc: 27.798% (7223/25984)     406/449 ==================>..] | Loss: 1.782 | Acc: 27.842% (7484/26880)     420/449 ====================>.] | Loss: 1.782 | Acc: 27.858% (7506/26944)     421/449 443/449 444/449 =================>] | Loss: 1.782 | Acc: 27.836% (7981/28672)     448/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.844% (8019/28800)     450/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.831% (8033/28864)   451/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.817% (8047/28928)     452/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.839% (8071/28992)     453/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.819% (8083/29056)     454/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.826% (8103/29120)     455/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.844% (8126/29184)     456/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.838% (8142/29248)     457/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.832% (8158/29312)     458/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.839% (8178/29376)     459/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.843% (8197/29440)     460/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.834% (8212/29504)     461/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.821% (8226/29568)     462/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.828% (8246/29632)     463/449 \n",
      " [==============================>] | Loss: 1.782 | Acc: 27.862% (8274/29696)     464/449 \n",
      " [===============================>] | Loss: 1.782 | Acc: 27.876% (8296/29760)     465/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.890% (8318/29824)   466/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.891% (8336/29888)   467/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.888% (8353/29952)   468/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.872% (8366/30016)     469/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.876% (8385/30080)     470/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.876% (8403/30144)     471/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.877% (8421/30208)   472/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.864% (8435/30272)     473/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.871% (8455/30336)     474/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.885% (8477/30400)     475/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.879% (8493/30464)     476/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.886% (8513/30528)   477/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.877% (8528/30592)   478/449 \n",
      " [===============================>] | Loss: 1.781 | Acc: 27.880% (8547/30656)     479/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.900% (8571/30720)     480/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.888% (8585/30784)   481/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.888% (8603/30848)     482/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.886% (8620/30912)     483/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.886% (8638/30976)     484/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.880% (8654/31040)     485/449 \n",
      " [================================>] | Loss: 1.781 | Acc: 27.887% (8674/31104)     486/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.884% (8691/31168)     487/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.888% (8710/31232)     488/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.911% (8735/31296)     489/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.905% (8751/31360)     490/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.918% (8773/31424)     491/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.928% (8794/31488)     492/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================>] | Loss: 1.780 | Acc: 27.932% (8813/31552)     493/449 \n",
      " [================================>] | Loss: 1.780 | Acc: 27.926% (8829/31616)     494/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.939% (8851/31680)     495/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.930% (8866/31744)   496/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.914% (8879/31808)     497/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.909% (8895/31872)     498/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.903% (8911/31936)     499/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.919% (8934/32000)     500/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.925% (8954/32064)     501/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.932% (8974/32128)     502/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.951% (8998/32192)     503/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.945% (9014/32256)     504/449 \n",
      " [=================================>] | Loss: 1.779 | Acc: 27.952% (9034/32320)     505/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.940% (9048/32384)     506/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.959% (9072/32448)     507/449 \n",
      " [=================================>] | Loss: 1.780 | Acc: 27.956% (9089/32512)     508/449 \n",
      " [=================================>] | Loss: 1.779 | Acc: 27.965% (9110/32576)     509/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.960% (9126/32640)     510/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.957% (9143/32704)     511/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.945% (9157/32768)     512/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.942% (9174/32832)     513/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.933% (9189/32896)     514/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.928% (9205/32960)   515/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.943% (9228/33024)     516/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.950% (9248/33088)     517/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.971% (9273/33152)     518/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.977% (9293/33216)     519/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.975% (9310/33280)     520/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.963% (9324/33344)     521/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.969% (9344/33408)   522/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.982% (9366/33472)     523/449 \n",
      " [==================================>] | Loss: 1.779 | Acc: 27.979% (9383/33536)     524/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.970% (9398/33600)     525/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.956% (9411/33664)     526/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.971% (9434/33728)     527/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.965% (9450/33792)     528/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.960% (9466/33856)     529/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.951% (9481/33920)     530/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.946% (9497/33984)     531/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.931% (9510/34048)     532/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.908% (9520/34112)     533/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.914% (9540/34176)     534/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.903% (9554/34240)     535/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.892% (9568/34304)     536/449 \n",
      " [===================================>] | Loss: 1.779 | Acc: 27.895% (9587/34368)     537/449 \n",
      " [===================================>] | Loss: 1.780 | Acc: 27.878% (9599/34432)     538/449 \n",
      " [===================================>] | Loss: 1.780 | Acc: 27.873% (9615/34496)     539/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.865% (9630/34560)     540/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.859% (9646/34624)     541/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.863% (9665/34688)     542/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.849% (9678/34752)     543/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.844% (9694/34816)     544/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.844% (9712/34880)   545/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.845% (9730/34944)     546/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.845% (9748/35008)     547/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.857% (9770/35072)     548/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.855% (9787/35136)     549/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.838% (9799/35200)     550/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.830% (9814/35264)     551/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.816% (9827/35328)     552/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.814% (9844/35392)     553/449 \n",
      " [====================================>] | Loss: 1.781 | Acc: 27.818% (9863/35456)     554/449 \n",
      " [=====================================>] | Loss: 1.782 | Acc: 27.796% (9873/35520)     555/449 \n",
      " [=====================================>] | Loss: 1.782 | Acc: 27.802% (9893/35584)     556/449 \n",
      " [=====================================>] | Loss: 1.782 | Acc: 27.791% (9907/35648)   557/449 \n",
      " [=====================================>] | Loss: 1.782 | Acc: 27.795% (9926/35712)     558/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.804% (9947/35776)     559/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.796% (9962/35840)     560/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.788% (9977/35904)     561/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.802% (10000/35968)    562/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.803% (10018/36032)    563/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.798% (10034/36096)    564/449 \n",
      " [=====================================>] | Loss: 1.782 | Acc: 27.777% (10044/36160)    565/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.785% (10065/36224)    566/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.786% (10083/36288)    567/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.792% (10103/36352)    568/449 \n",
      " [=====================================>] | Loss: 1.781 | Acc: 27.782% (10117/36416)    569/449 \n",
      " [======================================>] | Loss: 1.782 | Acc: 27.788% (10137/36480)    570/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.797% (10158/36544)    571/449 \n",
      " [======================================>] | Loss: 1.782 | Acc: 27.786% (10172/36608)    572/449 \n",
      " [======================================>] | Loss: 1.782 | Acc: 27.779% (10187/36672)    573/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.809% (10216/36736)    574/449 \n",
      " [======================================>] | Loss: 1.782 | Acc: 27.804% (10232/36800)    575/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.810% (10252/36864)    576/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.819% (10273/36928)    577/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.822% (10292/36992)    578/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.825% (10311/37056)    579/449 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================>] | Loss: 1.781 | Acc: 27.829% (10330/37120)    580/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.810% (10341/37184)    581/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.824% (10364/37248)    582/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.828% (10383/37312)  583/449 \n",
      " [======================================>] | Loss: 1.781 | Acc: 27.831% (10402/37376)    584/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.834% (10421/37440)    585/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.842% (10442/37504)    586/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.853% (10464/37568)    587/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.849% (10480/37632)    588/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.857% (10501/37696)    589/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.868% (10523/37760)    590/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.869% (10541/37824)  591/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.872% (10560/37888)  592/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.890% (10585/37952)    593/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.891% (10603/38016)    594/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.910% (10628/38080)    595/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.921% (10650/38144)    596/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.918% (10667/38208)    597/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.916% (10684/38272)    598/449 \n",
      " [=======================================>] | Loss: 1.781 | Acc: 27.922% (10704/38336)    599/449 \n",
      " [========================================>] | Loss: 1.781 | Acc: 27.938% (10728/38400)    600/449 \n",
      " [========================================>] | Loss: 1.781 | Acc: 27.943% (10748/38464)    601/449 \n",
      " [========================================>] | Loss: 1.781 | Acc: 27.933% (10762/38528)    602/449 \n",
      " [========================================>] | Loss: 1.781 | Acc: 27.936% (10781/38592)    603/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.947% (10803/38656)    604/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.960% (10826/38720)  605/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.968% (10847/38784)    606/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.971% (10866/38848)    607/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.968% (10883/38912)    608/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.976% (10904/38976)    609/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.987% (10926/39040)    610/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.984% (10943/39104)    611/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.979% (10959/39168)    612/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.967% (10972/39232)    613/449 \n",
      " [========================================>] | Loss: 1.780 | Acc: 27.980% (10995/39296)    614/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.965% (11007/39360)    615/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.973% (11028/39424)    616/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.971% (11045/39488)    617/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.966% (11061/39552)    618/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.958% (11076/39616)    619/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.954% (11092/39680)    620/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.954% (11110/39744)    621/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.952% (11127/39808)    622/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.947% (11143/39872)    623/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.965% (11168/39936)    624/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.978% (11191/40000)    625/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.968% (11205/40064)    626/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.978% (11227/40128)    627/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.978% (11245/40192)  628/449 \n",
      " [=========================================>] | Loss: 1.780 | Acc: 27.980% (11256/40229)    629/449 \n",
      "=====train_acc====== tensor(27.9798) =====epoch===== 7\n",
      " [=========================>....] | Loss: 1.742 | Acc: 29.655% (7231/24384)     381/449 ..............] | Loss: 1.771 | Acc: 27.344% (175/640)        10/44 15/449 ..........................] | Loss: 1.764 | Acc: 27.549% (335/1216)       19/449  24/449 .................] | Loss: 1.776 | Acc: 27.488% (475/1728)       27/449 ...........] | Loss: 1.780 | Acc: 27.009% (605/2240)       35/449 ...........] | Loss: 1.783 | Acc: 26.763% (668/2496)       39/44 40/449 ......] | Loss: 1.783 | Acc: 26.524% (696/2624)       41/449 ........................] | Loss: 1.779 | Acc: 26.285% (757/2880)       45/449 ............] | Loss: 1.783 | Acc: 26.590% (970/3648)       57/449 ..............] | Loss: 1.782 | Acc: 26.562% (986/3712)       58/449 61/449 .........................] | Loss: 1.768 | Acc: 27.344% (1190/4352)      68/44 70/449 ....] | Loss: 1.766 | Acc: 27.896% (1339/4800)      75/44 78/449 ......................] | Loss: 1.766 | Acc: 28.161% (1586/5632)      88/449 99/449 ...................] | Loss: 1.768 | Acc: 28.312% (1812/6400)      100/449 .] | Loss: 1.768 | Acc: 28.326% (1831/6464)      101/449    117/449 =>......................] | Loss: 1.761 | Acc: 28.972% (2188/7552)      118/44 121/449 122/449 ........] | Loss: 1.759 | Acc: 29.112% (2329/8000)      125/44 126/449 .........] | Loss: 1.760 | Acc: 28.986% (2356/8128)      127/449 =>.....................] | Loss: 1.759 | Acc: 28.979% (2374/8192)      128/449 ===>.....................] | Loss: 1.759 | Acc: 28.918% (2406/8320)      130/44 131/449 ................] | Loss: 1.759 | Acc: 28.871% (2439/8448)      132/449 ===>....................] | Loss: 1.761 | Acc: 28.463% (2696/9472)      148/449 150/44 151/449 >...................] | Loss: 1.762 | Acc: 28.442% (2785/9792)      153/449 ....] | Loss: 1.759 | Acc: 28.536% (2849/9984)      156/44 157/449 171/449 ..] | Loss: 1.759 | Acc: 28.402% (3181/11200)     175/449 181/449 ==========>.................] | Loss: 1.756 | Acc: 28.621% (3407/11904)     186/449 187/449 ......] | Loss: 1.756 | Acc: 28.624% (3444/12032)     188/449 ===>.................] | Loss: 1.753 | Acc: 28.814% (3596/12480)     195/449 201/44 202/449 .........] | Loss: 1.751 | Acc: 29.052% (3793/13056)     204/449     207/449 =============>................] | Loss: 1.750 | Acc: 29.147% (3880/13312)     208/449 =====>................] | Loss: 1.750 | Acc: 29.152% (3918/13440)     210/449 211/449 ............] | Loss: 1.749 | Acc: 29.198% (3999/13696)     214/449 ======>...............] | Loss: 1.750 | Acc: 29.033% (4125/14208)     222/449 228/449 =========>..............] | Loss: 1.748 | Acc: 29.171% (4294/14720)     230/449 ============>..............] | Loss: 1.748 | Acc: 29.153% (4310/14784)     231/449 ] | Loss: 1.747 | Acc: 29.178% (4407/15104)     236/449 ........] | Loss: 1.744 | Acc: 29.351% (4621/15744)     246/44 247/449 ......] | Loss: 1.744 | Acc: 29.366% (4661/15872)     248/44 253/44 255/449 ===>............] | Loss: 1.743 | Acc: 29.541% (4840/16384)     256/449 =========>............] | Loss: 1.744 | Acc: 29.505% (4853/16448)     257/449 ===============>............] | Loss: 1.744 | Acc: 29.536% (4877/16512)     258/44 268/44 276/449 =========>...........] | Loss: 1.744 | Acc: 29.671% (5355/18048)     282/449 ..] | Loss: 1.744 | Acc: 29.671% (5412/18240)     285/449 .] | Loss: 1.743 | Acc: 29.720% (5478/18432)     288/449 ===================>..........] | Loss: 1.744 | Acc: 29.640% (5615/18944)     296/449 ....] | Loss: 1.744 | Acc: 29.693% (5758/19392)     303/449 ....] | Loss: 1.744 | Acc: 29.672% (5849/19712)     308/449 ======>.........] | Loss: 1.744 | Acc: 29.708% (5932/19968)     312/449 314/44 316/449 320/449 ==================>........] | Loss: 1.744 | Acc: 29.721% (6125/20608)     322/449 ........] | Loss: 1.743 | Acc: 29.764% (6229/20928)     327/449 ===================>.......] | Loss: 1.742 | Acc: 29.772% (6345/21312)     333/44 338/449 =================>......] | Loss: 1.742 | Acc: 29.800% (6637/22272)     348/44 359/449 .....] | Loss: 1.744 | Acc: 29.658% (6966/23488)     367/44 370/449 ====================>.....] | Loss: 1.743 | Acc: 29.650% (7059/23808)     372/449 373/449 374/449 376/449 >....] | Loss: 1.742 | Acc: 29.667% (7177/24192)     378/44 379/449 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2808/3723400292.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2808/1617163977.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(epoah)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mcorrect\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mtotal\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m         \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2808/4234153052.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m         \u001B[0mimg_feature\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m         \u001B[0msingle_img_label\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    929\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    930\u001B[0m             \u001B[0mmaybe_callable\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_if_callable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 931\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_axis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmaybe_callable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    932\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    933\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_is_scalar_access\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_getitem_axis\u001B[1;34m(self, key, axis)\u001B[0m\n\u001B[0;32m   1566\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1567\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1568\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ixs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1569\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1570\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_get_slice_axis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslice_obj\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mslice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m_ixs\u001B[1;34m(self, i, axis)\u001B[0m\n\u001B[0;32m   3377\u001B[0m         \u001B[1;31m# irow\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3378\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0maxis\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3379\u001B[1;33m             \u001B[0mnew_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_mgr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfast_xs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3380\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3381\u001B[0m             \u001B[1;31m# if we are a copy, mark as such\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001B[0m in \u001B[0;36mfast_xs\u001B[1;34m(self, loc)\u001B[0m\n\u001B[0;32m    968\u001B[0m             \u001B[1;31m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    969\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrl\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mblk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmgr_locs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 970\u001B[1;33m                 \u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mrl\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mblk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    971\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    972\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExtensionDtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7add1056",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4406025503989659"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "data = pd.read_csv('deepfeature_2class.csv')\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(12,), random_state=42,\n",
    "    activation='relu')\n",
    "clf.fit(X, y)\n",
    "(clf.predict(X) == y).sum() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280190c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# train for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65423d08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8727598e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2808/4114285816.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtraindata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDatasetFromCSV\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'deepfeature_2class.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mtrain_loader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtraindata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m64\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"====End loading training data=====\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2808/4234153052.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, csv_path, transform)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcsv_path\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcsv_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    486\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    487\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 488\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    489\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1045\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1046\u001B[0m         \u001B[0mnrows\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"nrows\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1047\u001B[1;33m         \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcol_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1048\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1049\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    221\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlow_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m                 \u001B[0mchunks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_low_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m                 \u001B[1;31m# destructive to chunks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_concatenate_chunks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchunks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001B[0m in \u001B[0;36mis_extension_array_dtype\u001B[1;34m(arr_or_dtype)\u001B[0m\n\u001B[0;32m   1418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1419\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1420\u001B[1;33m \u001B[1;32mdef\u001B[0m \u001B[0mis_extension_array_dtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr_or_dtype\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1421\u001B[0m     \"\"\"\n\u001B[0;32m   1422\u001B[0m     \u001B[0mCheck\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0man\u001B[0m \u001B[0mobject\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0mextension\u001B[0m \u001B[0marray\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "traindata = DatasetFromCSV('deepfeature_2class.csv', transform=None)\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"====End loading training data=====\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}